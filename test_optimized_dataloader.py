#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æµ‹è¯•ä¼˜åŒ–åçš„æ•°æ®åŠ è½½å™¨æ€§èƒ½
"""

import os
import sys
import time
import psutil
import threading
import numpy as np

# æ·»åŠ è·¯å¾„
sys.path.append('./PatchTST_supervised')

def test_dataloader_performance():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨æ€§èƒ½"""
    print("=== æµ‹è¯•ä¼˜åŒ–åçš„æ•°æ®åŠ è½½å™¨æ€§èƒ½ ===")
    
    try:
        from data_provider.data_loader_for_inference import Dataset_Flight_Inference
        import torch
        from torch.utils.data import DataLoader
        
        # æ£€æŸ¥æ•°æ®æ–‡ä»¶
        data_path = "./PatchTST_supervised/dataset/processed_for_web/final_processed_trajectories.parquet"
        if not os.path.exists(data_path):
            print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_path}")
            return
        
        print(f"ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_path}")
        
        # æµ‹è¯•é…ç½®ï¼ˆåŸºäºæˆ‘ä»¬çš„ä¼˜åŒ–ï¼‰
        configs = [
            {"name": "åŸå§‹é…ç½®", "num_workers": 24, "batch_size": 512, "pin_memory": False, "persistent_workers": False},
            {"name": "ä¼˜åŒ–é…ç½®1", "num_workers": 4, "batch_size": 1024, "pin_memory": True, "persistent_workers": True},
            {"name": "ä¼˜åŒ–é…ç½®2", "num_workers": 4, "batch_size": 2048, "pin_memory": True, "persistent_workers": True},
            {"name": "ä¼˜åŒ–é…ç½®3", "num_workers": 6, "batch_size": 2048, "pin_memory": True, "persistent_workers": True},
        ]
        
        results = []
        
        for config in configs:
            print(f"\n--- æµ‹è¯• {config['name']} ---")
            print(f"é…ç½®: num_workers={config['num_workers']}, batch_size={config['batch_size']}")
            
            try:
                # åˆ›å»ºæ•°æ®é›†
                dataset_start = time.time()
                dataset = Dataset_Flight_Inference(
                    root_path="./PatchTST_supervised/dataset/processed_for_web/",
                    data_path="final_processed_trajectories.parquet",
                    size=[192, 0, 72],
                    features='M',
                    target='H',
                    scale=True,
                    timeenc=1,
                    freq='s'
                )
                dataset_time = time.time() - dataset_start
                print(f"æ•°æ®é›†åˆ›å»ºæ—¶é—´: {dataset_time:.2f}s")
                print(f"æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
                
                # åˆ›å»ºæ•°æ®åŠ è½½å™¨
                dataloader = DataLoader(
                    dataset,
                    batch_size=config['batch_size'],
                    shuffle=False,
                    num_workers=config['num_workers'],
                    drop_last=False,
                    pin_memory=config['pin_memory'],
                    persistent_workers=config['persistent_workers'] and config['num_workers'] > 0,
                    prefetch_factor=4 if config['num_workers'] > 0 else 2
                )
                
                print(f"æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼Œå…± {len(dataloader)} ä¸ªæ‰¹æ¬¡")
                
                # å¯åŠ¨CPUç›‘æ§
                cpu_usage = []
                memory_usage = []
                monitoring = True
                
                def monitor_system():
                    while monitoring:
                        cpu_percent = psutil.cpu_percent(interval=None, percpu=True)
                        active_cores = sum(1 for usage in cpu_percent if usage > 5.0)
                        memory = psutil.virtual_memory()
                        
                        cpu_usage.append({
                            'avg': np.mean(cpu_percent),
                            'max': np.max(cpu_percent),
                            'active_cores': active_cores
                        })
                        memory_usage.append(memory.percent)
                        time.sleep(0.5)
                
                monitor_thread = threading.Thread(target=monitor_system, daemon=True)
                monitor_thread.start()
                
                # æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½
                print("å¼€å§‹æ•°æ®åŠ è½½æµ‹è¯•...")
                load_times = []
                start_time = time.time()
                
                for i, batch in enumerate(dataloader):
                    batch_start = time.time()
                    
                    # æ¨¡æ‹Ÿæ•°æ®å¤„ç†
                    batch_x, batch_y, batch_x_mark, batch_y_mark, meta_info = batch
                    
                    # æ¨¡æ‹ŸGPUä¼ è¾“ï¼ˆå¦‚æœæœ‰GPUï¼‰
                    if torch.cuda.is_available():
                        batch_x = batch_x.float().cuda(non_blocking=True)
                        batch_x_mark = batch_x_mark.float().cuda(non_blocking=True)
                        torch.cuda.synchronize()
                    
                    batch_time = time.time() - batch_start
                    load_times.append(batch_time)
                    
                    # åªæµ‹è¯•å‰20ä¸ªæ‰¹æ¬¡
                    if i >= 19:
                        break
                
                # åœæ­¢ç›‘æ§
                monitoring = False
                monitor_thread.join(timeout=2)
                
                total_time = time.time() - start_time
                avg_batch_time = np.mean(load_times)
                
                # è®¡ç®—CPUç»Ÿè®¡
                if cpu_usage:
                    avg_cpu = np.mean([d['avg'] for d in cpu_usage])
                    max_cpu = np.max([d['max'] for d in cpu_usage])
                    avg_active_cores = np.mean([d['active_cores'] for d in cpu_usage])
                    avg_memory = np.mean(memory_usage)
                else:
                    avg_cpu = max_cpu = avg_active_cores = avg_memory = 0
                
                result = {
                    'config': config,
                    'dataset_time': dataset_time,
                    'total_time': total_time,
                    'avg_batch_time': avg_batch_time,
                    'throughput_batches': len(load_times) / total_time,
                    'throughput_samples': len(load_times) * config['batch_size'] / total_time,
                    'avg_cpu': avg_cpu,
                    'max_cpu': max_cpu,
                    'avg_active_cores': avg_active_cores,
                    'avg_memory': avg_memory
                }
                
                results.append(result)
                
                print(f"ç»“æœ:")
                print(f"  æ•°æ®é›†åˆå§‹åŒ–: {dataset_time:.2f}s")
                print(f"  æ€»åŠ è½½æ—¶é—´: {total_time:.2f}s")
                print(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.3f}s")
                print(f"  æ‰¹æ¬¡ååé‡: {result['throughput_batches']:.2f} batches/s")
                print(f"  æ ·æœ¬ååé‡: {result['throughput_samples']:.2f} samples/s")
                print(f"  å¹³å‡CPUä½¿ç”¨: {avg_cpu:.1f}%")
                print(f"  å¹³å‡æ´»è·ƒæ ¸å¿ƒ: {avg_active_cores:.1f}/24")
                print(f"  å¹³å‡å†…å­˜ä½¿ç”¨: {avg_memory:.1f}%")
                
            except Exception as e:
                print(f"é…ç½®æµ‹è¯•å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
            
            time.sleep(2)  # è®©ç³»ç»Ÿä¼‘æ¯
        
        # è¾“å‡ºå¯¹æ¯”ç»“æœ
        if results:
            print(f"\n=== æ€§èƒ½å¯¹æ¯”æ€»ç»“ ===")
            print(f"{'é…ç½®':<12} {'æ‰¹æ¬¡ååé‡':<12} {'æ ·æœ¬ååé‡':<12} {'CPUä½¿ç”¨':<10} {'æ´»è·ƒæ ¸å¿ƒ':<10}")
            print("-" * 70)
            
            best_config = None
            best_throughput = 0
            
            for result in results:
                config = result['config']
                throughput = result['throughput_samples']
                
                print(f"{config['name']:<12} {result['throughput_batches']:<12.2f} "
                      f"{throughput:<12.2f} {result['avg_cpu']:<10.1f} "
                      f"{result['avg_active_cores']:<10.1f}")
                
                if throughput > best_throughput:
                    best_throughput = throughput
                    best_config = config
            
            if best_config:
                print(f"\nğŸ† æœ€ä½³é…ç½®: {best_config['name']}")
                print(f"   num_workers: {best_config['num_workers']}")
                print(f"   batch_size: {best_config['batch_size']}")
                print(f"   æ ·æœ¬ååé‡: {best_throughput:.2f} samples/s")
        
    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_single_sample_performance():
    """æµ‹è¯•å•æ ·æœ¬è·å–æ€§èƒ½"""
    print("\n=== æµ‹è¯•å•æ ·æœ¬è·å–æ€§èƒ½ ===")
    
    try:
        from data_provider.data_loader_for_inference import Dataset_Flight_Inference
        
        # åˆ›å»ºæ•°æ®é›†
        print("åˆ›å»ºæ•°æ®é›†...")
        dataset = Dataset_Flight_Inference(
            root_path="./PatchTST_supervised/dataset/processed_for_web/",
            data_path="final_processed_trajectories.parquet",
            size=[192, 0, 72],
            features='M',
            target='H',
            scale=True,
            timeenc=1,
            freq='s'
        )
        
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
        
        # æµ‹è¯•å•æ ·æœ¬è·å–æ—¶é—´
        print("æµ‹è¯•å•æ ·æœ¬è·å–æ—¶é—´...")
        sample_times = []
        
        for i in range(min(100, len(dataset))):
            start_time = time.time()
            sample = dataset[i]
            sample_time = time.time() - start_time
            sample_times.append(sample_time)
        
        avg_sample_time = np.mean(sample_times)
        max_sample_time = np.max(sample_times)
        min_sample_time = np.min(sample_times)
        
        print(f"å•æ ·æœ¬è·å–æ€§èƒ½:")
        print(f"  å¹³å‡æ—¶é—´: {avg_sample_time:.4f}s")
        print(f"  æœ€å¤§æ—¶é—´: {max_sample_time:.4f}s")
        print(f"  æœ€å°æ—¶é—´: {min_sample_time:.4f}s")
        print(f"  å•æ ·æœ¬ååé‡: {1/avg_sample_time:.2f} samples/s")
        
        # åˆ†æç“¶é¢ˆ
        if avg_sample_time > 0.01:
            print("âš ï¸  å•æ ·æœ¬è·å–æ—¶é—´è¾ƒé•¿ï¼Œå¯èƒ½å­˜åœ¨__getitem__æ–¹æ³•ç“¶é¢ˆ")
        else:
            print("âœ… å•æ ·æœ¬è·å–æ€§èƒ½è‰¯å¥½")
            
    except Exception as e:
        print(f"å•æ ·æœ¬æµ‹è¯•å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æµ‹è¯•ä¼˜åŒ–åçš„æ•°æ®åŠ è½½å™¨æ€§èƒ½...")
    print(f"ç³»ç»Ÿä¿¡æ¯:")
    print(f"  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()} (é€»è¾‘)")
    print(f"  ç‰©ç†æ ¸å¿ƒæ•°: {psutil.cpu_count(logical=False)}")
    print(f"  æ€»å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    
    try:
        # æµ‹è¯•å•æ ·æœ¬æ€§èƒ½
        test_single_sample_performance()
        
        # æµ‹è¯•æ•°æ®åŠ è½½å™¨æ€§èƒ½
        test_dataloader_performance()
        
        print("\nğŸ¯ æµ‹è¯•å®Œæˆï¼")
        print("æ ¹æ®æµ‹è¯•ç»“æœé€‰æ‹©æœ€ä½³é…ç½®ä»¥è§£å†³å•çº¿ç¨‹CPUä½¿ç”¨é—®é¢˜ã€‚")
        
    except Exception as e:
        print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main()

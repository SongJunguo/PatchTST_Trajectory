使用context7 
你知道traffic库吧？这是traffic的源码工程。
下面这两个是我自己写的用于某个私有 的飞行轨迹数据的处理脚本。
my_flight_data_preprocessor_polars/flight_data_preprocessor_polars.py
my_flight_data_preprocessor_polars/run_preprocessing_polars.sh
现在的_smooth_and_clean_udf非常 的不好用。
经纬高里面0基本都是其实是没有意义的填充值，类似null或者nan。
我想考虑使用traffic自带的库函数，对轨迹进行插值重采样，滤波。也就是基本保持我现在脚本的功能，但是使用成熟的traffic库实现，你先告诉我有哪些方案？


你先考虑使用哪些traffic的函数能实现我要求的功能？可以阅读这个traffic的源码，可以使用context7

        pre_filter = FilterAboveSigmaMedian() | FilterMedian()
这里除了使用中值滤波，使用均值滤波合适吗?还有其他滤波吗？比如低通滤波？全面的考虑各种滤波


对于飞行轨迹，还是需要去除异常点的。

基于速度和加速度，最好是纯粹基于加速度的异常点检测。

去异常点还是必要的。

加速度阈值不应该用全局的，应该是基于一个窗口，自适应的阈值？但是数据有最低的阈值，防止因为噪声太多的轨迹被滤除太多的数据点，导致没有数据可用。

我有个疑问，计算速度是基于两个点的，速度能计算出左右两个点，那么是不是应该判断异常点，是不是基于左右两边的速度判断。

比如有一段平飞的轨迹，突然有3-5个异常的采样点很高，有点类似方波的异常超级短轨迹，能合理的滤出吗？原理是什么？

还有如果在一小段异常出现在轨迹的开头能滤出吗？

通过最小点数过滤短轨迹应该在最后处理完之后进行统计

可以考虑通过速度来筛去异常值点



# 飞行轨迹时间序列数据处理方案整理

基于您描述的数据处理需求，我帮您整理了一个完整的飞行轨迹数据清洗和预处理方案。以下是按照处理流程组织的详细要点：

## 数据概况

- **数据规模**：超过5.5GB，6000万个采样点
- **数据分布**：100个CSV文件
- **数据结构**：时间戳（微秒精度，实际只使用到毫秒精度，微秒精度都是0）、经纬度、高度、ID号
- **ID复用问题**：同一ID号可能对应不同时间段的不同航班
- **零值识别**：将经纬高中的存在部分采样点，有经纬但是高度为0，其实就是没有高度数据填充了0.所以需要把经纬高的零填充值转换为NaN或NULL
- **采样间隔不均匀** 数据采样可能是小于1秒钟采样一次（不确定有没有），也可能是大于1秒钟采样一次，3-4秒采样一次是常见的。但是我们希望最后得到1秒采样一次的数据，没有null和nan。
- **硬件配置**：128GB内存，96核骁龙CPU

### 2. 数据排序与分组
- **双重排序**：先按ID号排序，再按时间排序
- **ID分组**：将相同ID的记录归类到一起，为后续轨迹分割做准备

## 第二阶段：轨迹分割与唯一ID分配

### 3. 基于时间间隔的轨迹分割
- **时间差值计算**：计算同一ID内相邻采样点的时间间隔
- **分割阈值设定**：根据正常采样频率（1秒）设定合理的分割阈值，根据经验暂定20秒
- **轨迹断点检测**：识别时间间隔异常大的断点，将长轨迹分割为多个子轨迹
- **唯一ID生成**：为每个分割后的子轨迹分配新的唯一标识符（格式：原来id_新的轨迹的第一个采样点的时间戳）

### 4. 短轨迹过滤
- **长度阈值**：删除少于792个采样点的短轨迹

## 第四阶段：时间序列标准化

### 6. 上采样处理
- **统一采样频率**：将所有轨迹统一到1秒采样间隔[10][11]
- **时间网格对齐**：确保所有轨迹点在统一的时间网格上

### 7. 长缺失段处理
- **缺失长度检测**：识别超过15秒（15个采样点）的长缺失段[12]
- **轨迹二次分割**：在长缺失段两端切断轨迹，形成新的子轨迹
- **缺失段丢弃**：直接删除中间的长缺失段，不进行插值处理

## 第五阶段：数据插值与平滑

### 8. 线性插值
- **简单插值**：对短缺失段使用线性插值方法填充[13][14]
- **分维度处理**：分别对经度、纬度、高度进行插值计算

### 9. 卡尔曼滤波平滑
- **噪声去除**：使用卡尔曼滤波器去除轨迹数据中的噪声[15]
- **状态估计**：结合位置和速度信息进行最优状态估计
- **异常点平滑**：处理GPS信号异常导致的轨迹点漂移问题

## 第六阶段：数据格式转换与输出

### 10. 时间格式转换
- **数值化处理**：将时间戳转换为纯数字格式，便于深度学习模型处理
- **标准化时间基准**：统一时间起点和计量单位

### 11. 数据输出
- **CSV格式保存**：将处理后的数据保存为CSV格式
- **数据分片**：根据需要将大数据集分割为适合深度学习训练的小文件
- **元数据记录**：保存处理过程中的关键参数和统计信息

## 技术实现建议

### 内存管理优化
- 使用pandas的数据类型优化功能，将int64转换为更小的数据类型[1][2]
- 采用分块处理策略，避免将所有数据同时加载到内存中
- 及时释放不再使用的中间变量和数据框

### 并行处理策略
- 利用Dask库实现大数据的分布式处理[3]
- 充分利用96核CPU的并行计算能力
- 对独立的轨迹数据并行执行清洗和处理操作

### 质量控制措施
- 在每个处理阶段记录数据统计信息（如轨迹数量、采样点数量等）
- 实施数据完整性检查，确保处理过程中数据不丢失
- 保留处理前后的对比样本，验证处理效果的正确性

这个处理方案充分考虑了您的数据特点和硬件条件，通过系统化的步骤确保飞行轨迹数据的质量，为后续的深度学习应用提供高质量的训练数据。


第五阶段：零容忍最终验证与输出
设计动机: 这是整个流程的最终质量保证关卡。其存在的意义在于，我们不完全信任前面的所有自动化流程，而是以一种“怀疑一切”的态度，对即将输出的最终成果进行一次简单、粗暴但绝对有效的完整性检查。这确保了交付给下游用户或模型的数据是100%完美的。

详细操作步骤:

步骤5.1：最终完整性扫描

操作: 在所有处理步骤完成之后，在数据最终被写入文件之前，对每一条即将输出的轨迹进行最后一次遍历。

步骤5.2：执行“零容忍”检查

操作: 检查轨迹的经、纬、高三列中，是否还存在任何一个（哪怕只有一个）NaN值。

步骤5.3：处理与日志记录

如果未发现任何NaN: 恭喜，该轨迹通过了最严格的质量检验，予以保留，准备输出。

如果发现任何NaN:

发出最严重警告并记录日志: 立即向日志文件（）中写入一条[CRITICAL]级别的记录。日志内容应明确指出：“轨迹 [轨迹ID] 在完成所有处理步骤后，仍检测到残留的缺失值，已被强制丢弃。这强烈表明上游处理流程中存在未被覆盖的边缘案例，需要人工介入检查。”

彻底丢弃该条轨迹: 将这条被最终发现仍有瑕疵的轨迹从最终的数据集中完全移除，绝不让任何不完美的数据流入最终结果。

步骤5.4：输出

操作: 将所有通过了最终完整性质检的、100%纯净的轨迹数据，保存为CSV或其他指定格式的文件。

操作: 同时保存内容详尽的日志文件。这份日志是整个数据清洗过程的“黑匣子”，是未来进行问题排查、流程优化和质量评估不可或缺的宝贵资产。

PatchTST_supervised/data_provider/run_preprocessing_traffic.sh
PatchTST_supervised/data_provider/flight_data_preprocessor_traffic.py
根据上面的要求，仔细阅读上面py代码文件，深入思考，全面梳理逻辑，实现上面的功能。必要时候查阅使用context7查阅手册。




PatchTST_supervised/data_provider/flight_data_preprocessor_traffic.py
PatchTST_supervised/data_provider/run_preprocessing_traffic.sh

你仔细阅读代码。
我们通过运行debug发现，在这
        # 检查是否存在NaN或inf
        problematic_cols = []
        for col in key_cols_for_smoothing:
            # 直接在Pandas DataFrame上检查，而不是在NumPy数组上
            if (
                df_to_clean[col].isnull().any()
                or np.isinf(df_to_clean[col]).any()
            ):
                problematic_cols.append(col)

        if problematic_cols:
            logging.critical(
                f"严重警告: 轨迹ID '{unique_id}' 在送入卡尔曼滤波器前检测到NaN或inf值！"
                f" 这很可能导致 'matmul' 错误。问题列: {problematic_cols}"
            )
这之前的检测nan值的代码，包括这个，都没警告。

但是卡尔曼滤波之后，报了nan数值警告。

而且我们手工查看最后生成的csv，都是高度列在出问题，高度列在卡尔曼之后产生了nan。

我们怀疑是不是原始的数据，高度列数值长时间一样，不变化，导致的？

你可以查阅traffic的库函数源代码，这个工程包含的。也可以使用context7查阅相关库函数手册。
增加一个PLANETYPE检测，如果这个ID全是missing那就跳过，如果这个ID有一部分missing有一部分有值就去掉missing的部分
增加异常高度段检测，超过180



# -*- coding: utf-8 -*-
# ==============================================================================
# ** æ¨ç†ä¸“ç”¨æ•°æ®åŠ è½½å™¨ (Webå¯è§†åŒ–) **
#
# ** ç‰ˆæœ¬: 1.0 **
#
# ** åŠŸèƒ½: **
#   - ä»é¢„å¤„ç†å¥½çš„å†å²æ•°æ®æ–‡ä»¶(Parquet/CSV)ä¸­åŠ è½½æ•°æ®ã€‚
#   - ä¸ºæ¨¡å‹æ¨ç†ç”Ÿæˆæ»‘çª—æ•°æ®ã€‚
#   - åœ¨æ¯ä¸ªæ ·æœ¬ä¸­é™„åŠ å…ƒæ•°æ®(meta_info)ï¼Œç”¨äºå°†é¢„æµ‹ç»“æœä¸å†å²æ•°æ®å…³è”ã€‚
#
# ==============================================================================

import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from sklearn.preprocessing import StandardScaler
from utils.timefeatures import time_features
import warnings

warnings.filterwarnings('ignore')

class Dataset_Flight_Inference(Dataset):
    def __init__(self, root_path, data_path='history_data.parquet', size=None,
                 features='M', target='H', scale=True, timeenc=1, freq='s', stride=1, stats_path=None):
        """
        Args:
            root_path (str): æ•°æ®æ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½•ã€‚
            data_path (str): æ•°æ®æ–‡ä»¶åã€‚
            size (list): [seq_len, label_len, pred_len]ã€‚
            features (str): 'M' (å¤šå˜é‡) æˆ– 'S' (å•å˜é‡)ã€‚
            target (str): å•å˜é‡é¢„æµ‹æ—¶çš„ç›®æ ‡åˆ—ã€‚
            scale (bool): æ˜¯å¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚
            timeenc (int): æ—¶é—´ç¼–ç æ–¹å¼ (0 for simple, 1 for detailed features)ã€‚
            freq (str): æ—¶é—´ç‰¹å¾ç¼–ç çš„é¢‘ç‡ã€‚
            stride (int): æ•°æ®åŠ è½½å™¨çš„æ»‘çª—æ­¥é•¿ã€‚
            stats_path (str): å½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶çš„è·¯å¾„ã€‚
        """
        # 1. åˆå§‹åŒ–åŸºæœ¬å‚æ•°
        if size is None:
            self.seq_len = 96
            self.label_len = 0  # æ¨ç†æ—¶é€šå¸¸ä¸éœ€è¦
            self.pred_len = 72
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq
        self.stride = stride # ğŸš€ ä¿å­˜æ­¥é•¿
        self.stats_path = stats_path # ä¿å­˜ç»Ÿè®¡æ–‡ä»¶è·¯å¾„

        self.root_path = root_path
        self.data_path = data_path
        
        # 2. è¯»å–å’Œé¢„å¤„ç†æ•°æ®
        self.__read_data__()

    def __read_data__(self):
        file_path = os.path.join(self.root_path, self.data_path)
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
        if self.data_path.endswith('.parquet'):
            df_full = pd.read_parquet(file_path)
        else:
            df_full = pd.read_csv(file_path)

        # --- å…¼å®¹å±‚ï¼šå¤„ç† JD/WD å’Œ Lon/Lat åˆ—åä¸ä¸€è‡´çš„é—®é¢˜ ---
        rename_map = {}
        if 'JD' in df_full.columns and 'Lon' not in df_full.columns:
            rename_map['JD'] = 'Lon'
        if 'WD' in df_full.columns and 'Lat' not in df_full.columns:
            rename_map['WD'] = 'Lat'
        
        if rename_map:
            df_full.rename(columns=rename_map, inplace=True)
            print(f"ä¸ºå…¼å®¹æ—§æ•°æ®æ ¼å¼ï¼Œå·²æ‰§è¡Œåˆ—åé‡å‘½å: {rename_map}")

        # --- æ ‡å‡†åŒ–ä¸ç‰¹å¾é€‰æ‹© (ç»Ÿä¸€é€»è¾‘) ---
        if not (self.stats_path and os.path.exists(self.stats_path)):
            raise FileNotFoundError(f"å½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶æœªæ‰¾åˆ°æˆ–æœªæä¾›è·¯å¾„: {self.stats_path}")

        # 1. ä»ç»Ÿè®¡æ–‡ä»¶åŠ è½½æƒå¨çš„ç‰¹å¾åˆ—è¡¨
        stats_df = pd.read_csv(self.stats_path)
        stats_features = list(stats_df['feature'])
        print(f"ä»ç»Ÿè®¡æ–‡ä»¶åŠ è½½çš„æƒå¨ç‰¹å¾åˆ—è¡¨: {stats_features}")

        # 2. æ£€æŸ¥æ•°æ®æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦çš„ç‰¹å¾
        missing_features = [f for f in stats_features if f not in df_full.columns]
        if missing_features:
            raise ValueError(f"æ•°æ®æ–‡ä»¶ {self.data_path} (æˆ–é‡å‘½åå) ç¼ºå°‘ä»¥ä¸‹å¿…è¦çš„ç‰¹å¾: {missing_features}")

        # 3. ã€æ ¸å¿ƒã€‘ç›´æ¥æ ¹æ®ç»Ÿè®¡æ–‡ä»¶çš„ç‰¹å¾åˆ—è¡¨æ¥é€‰æ‹©æ•°æ®
        df_data = df_full[stats_features].copy()
        print(f"å·²æ ¹æ®ç»Ÿè®¡æ–‡ä»¶æˆåŠŸç­›é€‰å‡ºç‰¹å¾åˆ—: {list(df_data.columns)}")

        # 4. æ ¹æ® scale å‚æ•°å†³å®šæ˜¯å¦è¿›è¡Œå½’ä¸€åŒ–
        self.scaler = StandardScaler()
        if self.scale:
            self.scaler.mean_ = stats_df['mean'].values
            self.scaler.scale_ = stats_df['std'].values
            print(f"æˆåŠŸä» {self.stats_path} åŠ è½½å¹¶åº”ç”¨å½’ä¸€åŒ–ç»Ÿè®¡ä¿¡æ¯ã€‚")
            data = self.scaler.transform(df_data.values.astype(np.float32))
        else:
            # å¦‚æœä¸å½’ä¸€åŒ–ï¼Œä¹Ÿä½¿ç”¨æ­£ç¡®é€‰æ‹©çš„ç‰¹å¾ï¼Œä½†åªå–å…¶åŸå§‹å€¼
            print("scale=Falseï¼Œè·³è¿‡å½’ä¸€åŒ–æ­¥éª¤ã€‚")
            data = df_data.values.astype(np.float32)

        # --- æ—¶é—´ç‰¹å¾ç¼–ç  ---
        df_stamp = df_full[['Time']]
        df_stamp.rename(columns={'Time': 'date'}, inplace=True)
        df_stamp['date'] = pd.to_datetime(df_stamp.date)
        
        if self.timeenc == 1:
            # å°† Series è½¬æ¢ä¸º DatetimeIndex
            datetime_index = pd.DatetimeIndex(df_stamp['date'])
            data_stamp = time_features(datetime_index, freq=self.freq)
            data_stamp = data_stamp.transpose(1, 0)
        else: # ç®€æ˜“æ—¶é—´ç¼–ç 
            df_stamp['month'] = df_stamp.date.dt.month
            df_stamp['day'] = df_stamp.date.dt.day
            df_stamp['weekday'] = df_stamp.date.dt.weekday
            df_stamp['hour'] = df_stamp.date.dt.hour
            data_stamp = df_stamp.drop(['date'], axis=1).values

        # --- ğŸš€ ä¼˜åŒ–ï¼šæ„å»ºç´¢å¼•æ˜ å°„ ---
        self.data_x = []
        self.data_stamp = []
        self.meta_data = [] # å­˜å‚¨å…ƒæ•°æ®
        self.index_mapping = []

        print(f"æ­£åœ¨å¤„ç† {df_full['ID'].nunique()} æ¡è½¨è¿¹çš„æ•°æ®...")

        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„groupbyå¤„ç†
        grouped = df_full.groupby('ID', sort=False)  # sort=False å¯ä»¥æé«˜æ€§èƒ½

        for traj_idx, (name, group) in enumerate(grouped):
            # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨è¿ç»­ç´¢å¼•ï¼Œé¿å…å¤æ‚çš„ç´¢å¼•æ“ä½œ
            if group.index.is_monotonic_increasing:
                # å¦‚æœç´¢å¼•æ˜¯è¿ç»­çš„ï¼Œç›´æ¥åˆ‡ç‰‡
                start_idx = group.index[0]
                end_idx = group.index[-1] + 1
                group_data = data[start_idx:end_idx]
                stamp_data = data_stamp[start_idx:end_idx]
            else:
                # å¦‚æœç´¢å¼•ä¸è¿ç»­ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•
                group_indices = group.index
                group_data = data[group_indices]
                stamp_data = data_stamp[group_indices]

            # è·å–å…ƒæ•°æ®ï¼ˆä¼˜åŒ–ï¼šå‡å°‘ilocè°ƒç”¨ï¼‰
            first_row = group.iloc[0]
            meta_group = {
                'ID': name,
                'Time': group['Time'].values # å­˜å‚¨æ•´ä¸ªè½¨è¿¹çš„æ—¶é—´æˆ³
            }

            self.data_x.append(group_data)
            self.data_stamp.append(stamp_data)
            self.meta_data.append(meta_group)

            # è®¡ç®—è¯¥è½¨è¿¹å¯ä»¥äº§ç”Ÿçš„æ ·æœ¬æ•°é‡
            traj_samples = len(group_data) - self.seq_len + 1
            if traj_samples > 0:
                # ğŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡æ·»åŠ ç´¢å¼•æ˜ å°„ï¼Œå¹¶ä½¿ç”¨æ­¥é•¿
                traj_indices = [(traj_idx, local_idx) for local_idx in range(0, traj_samples, self.stride)]
                self.index_mapping.extend(traj_indices)

        print(f"æ•°æ®å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(self.index_mapping)} ä¸ªæ ·æœ¬ (æ»‘çª—æ­¥é•¿: {self.stride})")

    def __getitem__(self, index):
        # 1. ä½¿ç”¨é¢„è®¡ç®—çš„ç´¢å¼•æ˜ å°„ï¼ŒO(1)å¤æ‚åº¦
        traj_idx, local_idx = self.index_mapping[index]

        # 2. è·å–å¯¹åº”è½¨è¿¹çš„æ•°æ®ï¼ˆä¼˜åŒ–ï¼šç›´æ¥ç´¢å¼•ï¼Œé¿å…é¢å¤–å˜é‡ï¼‰
        traj_data_x = self.data_x[traj_idx]
        traj_data_stamp = self.data_stamp[traj_idx]
        traj_meta = self.meta_data[traj_idx]

        # 3. å®šä¹‰æ»‘çª—ï¼ˆä¼˜åŒ–ï¼šç›´æ¥è®¡ç®—ï¼Œå‡å°‘å˜é‡èµ‹å€¼ï¼‰
        s_begin = local_idx
        s_end = s_begin + self.seq_len

        # 4. ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è§†å›¾è€Œéå¤åˆ¶ï¼Œå‡å°‘å†…å­˜å¼€é”€
        seq_x = traj_data_x[s_begin:s_end]  # ä½¿ç”¨è§†å›¾ï¼Œé¿å…ä¸å¿…è¦çš„å¤åˆ¶
        seq_x_mark = traj_data_stamp[s_begin:s_end]  # ä½¿ç”¨è§†å›¾

        # 5. å‡†å¤‡è¦ä¼ é€’çš„å…ƒæ•°æ®ï¼ˆä¼˜åŒ–ï¼šå‡å°‘å­—å…¸æŸ¥æ‰¾ï¼‰
        prediction_anchor_time = traj_meta['Time'][s_end - 1]

        meta_info = {
            'Pred_trajectory_id': traj_meta['ID'],
            'prediction_anchor_time': prediction_anchor_time
        }

        # 6. ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨é¢„åˆ†é…çš„å…¨å±€é›¶æ•°ç»„ï¼Œé¿å…é‡å¤åˆ›å»º
        if not hasattr(self, '_seq_y_template'):
            self._seq_y_template = np.zeros((self.pred_len, seq_x.shape[-1]), dtype=np.float32)
            self._seq_y_mark_template = np.zeros((self.pred_len, seq_x_mark.shape[-1]), dtype=np.float32)

        # è¿”å›æ¨¡æ¿çš„å‰¯æœ¬ï¼ˆæ¯”æ¯æ¬¡åˆ›å»ºæ–°æ•°ç»„å¿«ï¼‰
        seq_y = self._seq_y_template.copy()
        seq_y_mark = self._seq_y_mark_template.copy()

        return seq_x, seq_y, seq_x_mark, seq_y_mark, meta_info

    def __len__(self):
        return len(self.index_mapping)

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)

    def get_scaler(self):
        return self.scaler
# -*- coding: utf-8 -*-
# ==============================================================================
# ** æ¨ç†ä¸“ç”¨æ•°æ®åŠ è½½å™¨ (Webå¯è§†åŒ–) **
#
# ** ç‰ˆæœ¬: 1.0 **
#
# ** åŠŸèƒ½: **
#   - ä»é¢„å¤„ç†å¥½çš„å†å²æ•°æ®æ–‡ä»¶(Parquet/CSV)ä¸­åŠ è½½æ•°æ®ã€‚
#   - ä¸ºæ¨¡å‹æ¨ç†ç”Ÿæˆæ»‘çª—æ•°æ®ã€‚
#   - åœ¨æ¯ä¸ªæ ·æœ¬ä¸­é™„åŠ å…ƒæ•°æ®(meta_info)ï¼Œç”¨äºå°†é¢„æµ‹ç»“æœä¸å†å²æ•°æ®å…³è”ã€‚
#
# ==============================================================================

import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from sklearn.preprocessing import StandardScaler
from utils.timefeatures import time_features
import warnings

warnings.filterwarnings('ignore')

class Dataset_Flight_Inference(Dataset):
    def __init__(self, root_path, data_path='history_data.parquet', size=None,
                 features='M', target='H', scale=True, timeenc=1, freq='s', stride=1):
        """
        Args:
            root_path (str): æ•°æ®æ–‡ä»¶æ‰€åœ¨çš„æ ¹ç›®å½•ã€‚
            data_path (str): æ•°æ®æ–‡ä»¶åã€‚
            size (list): [seq_len, label_len, pred_len]ã€‚
            features (str): 'M' (å¤šå˜é‡) æˆ– 'S' (å•å˜é‡)ã€‚
            target (str): å•å˜é‡é¢„æµ‹æ—¶çš„ç›®æ ‡åˆ—ã€‚
            scale (bool): æ˜¯å¦å¯¹æ•°æ®è¿›è¡Œæ ‡å‡†åŒ–ã€‚
            timeenc (int): æ—¶é—´ç¼–ç æ–¹å¼ (0 for simple, 1 for detailed features)ã€‚
            freq (str): æ—¶é—´ç‰¹å¾ç¼–ç çš„é¢‘ç‡ã€‚
            stride (int): æ•°æ®åŠ è½½å™¨çš„æ»‘çª—æ­¥é•¿ã€‚
        """
        # 1. åˆå§‹åŒ–åŸºæœ¬å‚æ•°
        if size is None:
            self.seq_len = 96
            self.label_len = 0  # æ¨ç†æ—¶é€šå¸¸ä¸éœ€è¦
            self.pred_len = 72
        else:
            self.seq_len = size[0]
            self.label_len = size[1]
            self.pred_len = size[2]

        self.features = features
        self.target = target
        self.scale = scale
        self.timeenc = timeenc
        self.freq = freq
        self.stride = stride # ğŸš€ ä¿å­˜æ­¥é•¿

        self.root_path = root_path
        self.data_path = data_path
        
        # 2. è¯»å–å’Œé¢„å¤„ç†æ•°æ®
        self.__read_data__()

    def __read_data__(self):
        file_path = os.path.join(self.root_path, self.data_path)
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©è¯»å–æ–¹å¼
        if self.data_path.endswith('.parquet'):
            df_full = pd.read_parquet(file_path)
        else:
            df_full = pd.read_csv(file_path)

        # --- ç‰¹å¾é€‰æ‹© ---
        # è·å–é™¤IDå’ŒTimeä¹‹å¤–çš„æ‰€æœ‰æ•°å€¼åˆ—
        cols = list(df_full.columns)
        non_feature_cols = ['ID', 'Time', 'PARTNO', 'P1', 'GP', 'TASK', 'PLANETYPE']
        feature_cols = [c for c in cols if c not in non_feature_cols]

        if self.features == 'M':
            cols_data = feature_cols
        elif self.features == 'S':
            cols_data = [self.target]
        else: # MS
            cols_data = feature_cols

        df_data = df_full[cols_data]
        
        # --- æ ‡å‡†åŒ–ï¼ˆä¼˜åŒ–ï¼šä½¿ç”¨float32å‡å°‘å†…å­˜ä½¿ç”¨ï¼‰ ---
        self.scaler = StandardScaler()
        if self.scale:
            self.scaler.fit(df_data.values.astype(np.float32))
            data = self.scaler.transform(df_data.values.astype(np.float32))
        else:
            data = df_data.values.astype(np.float32)

        # --- æ—¶é—´ç‰¹å¾ç¼–ç  ---
        df_stamp = df_full[['Time']]
        df_stamp.rename(columns={'Time': 'date'}, inplace=True)
        df_stamp['date'] = pd.to_datetime(df_stamp.date)
        
        if self.timeenc == 1:
            # å°† Series è½¬æ¢ä¸º DatetimeIndex
            datetime_index = pd.DatetimeIndex(df_stamp['date'])
            data_stamp = time_features(datetime_index, freq=self.freq)
            data_stamp = data_stamp.transpose(1, 0)
        else: # ç®€æ˜“æ—¶é—´ç¼–ç 
            df_stamp['month'] = df_stamp.date.dt.month
            df_stamp['day'] = df_stamp.date.dt.day
            df_stamp['weekday'] = df_stamp.date.dt.weekday
            df_stamp['hour'] = df_stamp.date.dt.hour
            data_stamp = df_stamp.drop(['date'], axis=1).values

        # --- ğŸš€ ä¼˜åŒ–ï¼šæ„å»ºç´¢å¼•æ˜ å°„ ---
        self.data_x = []
        self.data_stamp = []
        self.meta_data = [] # å­˜å‚¨å…ƒæ•°æ®
        self.index_mapping = []

        print(f"æ­£åœ¨å¤„ç† {df_full['ID'].nunique()} æ¡è½¨è¿¹çš„æ•°æ®...")

        # ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨æ›´é«˜æ•ˆçš„groupbyå¤„ç†
        grouped = df_full.groupby('ID', sort=False)  # sort=False å¯ä»¥æé«˜æ€§èƒ½

        for traj_idx, (name, group) in enumerate(grouped):
            # ğŸš€ ä¼˜åŒ–ï¼šç›´æ¥ä½¿ç”¨è¿ç»­ç´¢å¼•ï¼Œé¿å…å¤æ‚çš„ç´¢å¼•æ“ä½œ
            if group.index.is_monotonic_increasing:
                # å¦‚æœç´¢å¼•æ˜¯è¿ç»­çš„ï¼Œç›´æ¥åˆ‡ç‰‡
                start_idx = group.index[0]
                end_idx = group.index[-1] + 1
                group_data = data[start_idx:end_idx]
                stamp_data = data_stamp[start_idx:end_idx]
            else:
                # å¦‚æœç´¢å¼•ä¸è¿ç»­ï¼Œä½¿ç”¨åŸæ¥çš„æ–¹æ³•
                group_indices = group.index
                group_data = data[group_indices]
                stamp_data = data_stamp[group_indices]

            # è·å–å…ƒæ•°æ®ï¼ˆä¼˜åŒ–ï¼šå‡å°‘ilocè°ƒç”¨ï¼‰
            first_row = group.iloc[0]
            meta_group = {
                'ID': name,
                'TASK': first_row['TASK'],
                'PLANETYPE': first_row['PLANETYPE'],
                'Time': group['Time'].values # å­˜å‚¨æ•´ä¸ªè½¨è¿¹çš„æ—¶é—´æˆ³
            }

            self.data_x.append(group_data)
            self.data_stamp.append(stamp_data)
            self.meta_data.append(meta_group)

            # è®¡ç®—è¯¥è½¨è¿¹å¯ä»¥äº§ç”Ÿçš„æ ·æœ¬æ•°é‡
            traj_samples = len(group_data) - self.seq_len + 1
            if traj_samples > 0:
                # ğŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡æ·»åŠ ç´¢å¼•æ˜ å°„ï¼Œå¹¶ä½¿ç”¨æ­¥é•¿
                traj_indices = [(traj_idx, local_idx) for local_idx in range(0, traj_samples, self.stride)]
                self.index_mapping.extend(traj_indices)

        print(f"æ•°æ®å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(self.index_mapping)} ä¸ªæ ·æœ¬ (æ»‘çª—æ­¥é•¿: {self.stride})")

    def __getitem__(self, index):
        # 1. ä½¿ç”¨é¢„è®¡ç®—çš„ç´¢å¼•æ˜ å°„ï¼ŒO(1)å¤æ‚åº¦
        traj_idx, local_idx = self.index_mapping[index]

        # 2. è·å–å¯¹åº”è½¨è¿¹çš„æ•°æ®ï¼ˆä¼˜åŒ–ï¼šç›´æ¥ç´¢å¼•ï¼Œé¿å…é¢å¤–å˜é‡ï¼‰
        traj_data_x = self.data_x[traj_idx]
        traj_data_stamp = self.data_stamp[traj_idx]
        traj_meta = self.meta_data[traj_idx]

        # 3. å®šä¹‰æ»‘çª—ï¼ˆä¼˜åŒ–ï¼šç›´æ¥è®¡ç®—ï¼Œå‡å°‘å˜é‡èµ‹å€¼ï¼‰
        s_begin = local_idx
        s_end = s_begin + self.seq_len

        # 4. ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨è§†å›¾è€Œéå¤åˆ¶ï¼Œå‡å°‘å†…å­˜å¼€é”€
        seq_x = traj_data_x[s_begin:s_end]  # ä½¿ç”¨è§†å›¾ï¼Œé¿å…ä¸å¿…è¦çš„å¤åˆ¶
        seq_x_mark = traj_data_stamp[s_begin:s_end]  # ä½¿ç”¨è§†å›¾

        # 5. å‡†å¤‡è¦ä¼ é€’çš„å…ƒæ•°æ®ï¼ˆä¼˜åŒ–ï¼šå‡å°‘å­—å…¸æŸ¥æ‰¾ï¼‰
        prediction_anchor_time = traj_meta['Time'][s_end - 1]

        meta_info = {
            'Pred_trajectory_id': traj_meta['ID'],
            'prediction_anchor_time': prediction_anchor_time,
            'TASK': traj_meta['TASK'],
            'PLANETYPE': traj_meta['PLANETYPE']
        }

        # 6. ğŸš€ ä¼˜åŒ–ï¼šä½¿ç”¨é¢„åˆ†é…çš„å…¨å±€é›¶æ•°ç»„ï¼Œé¿å…é‡å¤åˆ›å»º
        if not hasattr(self, '_seq_y_template'):
            self._seq_y_template = np.zeros((self.pred_len, seq_x.shape[-1]), dtype=np.float32)
            self._seq_y_mark_template = np.zeros((self.pred_len, seq_x_mark.shape[-1]), dtype=np.float32)

        # è¿”å›æ¨¡æ¿çš„å‰¯æœ¬ï¼ˆæ¯”æ¯æ¬¡åˆ›å»ºæ–°æ•°ç»„å¿«ï¼‰
        seq_y = self._seq_y_template.copy()
        seq_y_mark = self._seq_y_mark_template.copy()

        return seq_x, seq_y, seq_x_mark, seq_y_mark, meta_info

    def __len__(self):
        return len(self.index_mapping)

    def inverse_transform(self, data):
        return self.scaler.inverse_transform(data)

    def get_scaler(self):
        return self.scaler
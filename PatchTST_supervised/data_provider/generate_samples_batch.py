import os
import shutil # ç”¨äºé«˜æ•ˆçš„æ–‡ä»¶æ“ä½œ
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from concurrent.futures import ProcessPoolExecutor, as_completed
from tqdm import tqdm
import time

# =========================================================
# æ‰€æœ‰çš„è¾…åŠ©å‡½æ•°å’Œå·¥ä½œå‡½æ•°éƒ½å¿…é¡»åœ¨è¿™é‡Œå®šä¹‰
# =========================================================

def z_score_normalize(data, means, stds):
    """Z-scoreå½’ä¸€åŒ–"""
    stds[stds == 0] = 1.0
    return (data - means) / stds

def append_to_csv(filepath, data_array):
    """å°†Numpyæ•°ç»„è¿½åŠ åˆ°CSVæ–‡ä»¶ï¼Œä¸å«è¡¨å¤´å’Œç´¢å¼•ã€‚"""
    if data_array.size == 0:
        return
    data_to_save = data_array.reshape(-1, data_array.shape[-1])
    # 'a'æ¨¡å¼ä¸ºè¿½åŠ ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™åˆ›å»º
    pd.DataFrame(data_to_save).to_csv(filepath, mode='a', header=False, index=False)

def process_file(file_path):
    """(å¹¶è¡Œå·¥ä½œå‡½æ•°) è¯»å–å•ä¸ªCSVæ–‡ä»¶ï¼Œè¿”å›è½¨è¿¹å­—å…¸ã€‚"""
    try:
        df = pd.read_csv(file_path, dtype={0: str}, low_memory=False)
        df_data = df.iloc[:, :4]
        df_data.columns = ['id', 'lon', 'lat', 'h']
        trajectories = {}
        for traj_id, group in df_data.groupby('id'):
            trajectories[traj_id] = group[['lon', 'lat', 'h']].values.astype(np.float32)
        return trajectories
    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
        return {}

def create_windows_and_save_to_temp(args):
    """
    (æœ€ç»ˆç‰ˆå¹¶è¡Œå·¥ä½œå‡½æ•°) 
    ä¸ºå•ä¸ªè½¨è¿¹åˆ›å»ºçª—å£ï¼Œå¹¶ç›´æ¥å°†ç»“æœä¿å­˜åˆ°å”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶ä¸­ã€‚
    è¿”å›ä¸´æ—¶æ–‡ä»¶çš„è·¯å¾„å’Œç»Ÿè®¡ä¿¡æ¯ï¼Œè€Œä¸æ˜¯å·¨å¤§çš„æ•°æ®æœ¬èº«ã€‚
    """
    (traj_id, traj_data, sequence_length, prediction_steps, time_step, 
     means, stds, temp_dir, set_name) = args
    
    # åˆ›å»ºçª—å£
    total_length = sequence_length + prediction_steps
    windows = []
    for i in range(0, len(traj_data) - total_length + 1, time_step):
        window = traj_data[i : i + total_length, :].copy()
        normalized_window = z_score_normalize(window, means, stds)
        windows.append(normalized_window)
    
    if not windows:
        return None

    windows_array = np.array(windows, dtype=np.float32)
    
    # å®šä¹‰å”¯ä¸€çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    temp_x_path = os.path.join(temp_dir, f"{set_name}_x_{os.getpid()}_{traj_id}.csv")
    temp_y_path = os.path.join(temp_dir, f"{set_name}_y_{os.getpid()}_{traj_id}.csv")
    
    # ç›´æ¥å†™å…¥ä¸´æ—¶æ–‡ä»¶
    append_to_csv(temp_x_path, windows_array[:, :sequence_length, :])
    append_to_csv(temp_y_path, windows_array[:, sequence_length:, :])
    
    # åªè¿”å›è½»é‡çº§çš„ç»“æœ
    return {
        'set_name': set_name,
        'x_path': temp_x_path,
        'y_path': temp_y_path,
        'id': traj_id,
        'count': len(windows)
    }

# =========================================================
# æœ€ç»ˆçš„ä¸»å‡½æ•°
# =========================================================
def process_all_files_final(file_path, save_path, sequence_length, prediction_steps, time_step, num_workers):
    """
    æœ€ç»ˆç‰ˆï¼šå¹¶è¡Œè®¡ç®—å¹¶ç”±å·¥ä½œè¿›ç¨‹ç›´æ¥å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œä¸»è¿›ç¨‹æœ€ååˆå¹¶ã€‚
    """
    print(f"ğŸš€ æœ€ç»ˆç‰ˆå¹¶è¡Œå¤„ç† (æœ€å¤š{num_workers}æ ¸)...")
    
    # å‡†å¤‡ä¸€ä¸ªä¸´æ—¶ç›®å½•æ¥å­˜æ”¾ä¸­é—´æ–‡ä»¶
    temp_dir = os.path.join(save_path, "temp_files")
    if os.path.exists(temp_dir):
        shutil.rmtree(temp_dir)
    os.makedirs(temp_dir)
    
    # æ¸…ç†æœ€ç»ˆçš„è¾“å‡ºæ–‡ä»¶
    output_files_paths = {
        'train_x': os.path.join(save_path, 'train_x.csv'), 'train_y': os.path.join(save_path, 'train_y.csv'),
        'val_x': os.path.join(save_path, 'val_x.csv'), 'val_y': os.path.join(save_path, 'val_y.csv'),
        'test_x': os.path.join(save_path, 'test_x.csv'), 'test_y': os.path.join(save_path, 'test_y.csv'),
        'train_ids': os.path.join(save_path, 'train_ids.csv'), 'val_ids': os.path.join(save_path, 'val_ids.csv'),
        'test_ids': os.path.join(save_path, 'test_ids.csv')
    }
    for f in output_files_paths.values():
        if os.path.exists(f):
            os.remove(f)

    # æ­¥éª¤ 1 & 2: è¯»å–æ•°æ®å’Œè®¡ç®—ç»Ÿè®¡é‡
    print("\n[æ­¥éª¤ 1 & 2: è¯»å–æ•°æ®ä¸è®¡ç®—ç»Ÿè®¡é‡...]")
    all_files = []
    if os.path.isdir(file_path):
        all_files = [os.path.join(file_path, f) for f in os.listdir(file_path) if f.endswith('.csv')]
    elif os.path.isfile(file_path) and file_path.endswith('.csv'):
        all_files = [file_path]
    if not all_files: 
        print(f"é”™è¯¯ï¼šåœ¨è·¯å¾„ '{file_path}' ä¸­æœªæ‰¾åˆ°ä»»ä½• .csv æ–‡ä»¶ã€‚")
        return False

    all_trajectories_dict = {}
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        futures = {executor.submit(process_file, f): f for f in all_files}
        for future in tqdm(as_completed(futures), total=len(all_files), desc="è¯»å–æ–‡ä»¶"):
            all_trajectories_dict.update(future.result())
    
    all_ids = list(all_trajectories_dict.keys())
    train_ids, temp_ids = train_test_split(all_ids, test_size=0.3, random_state=42)
    val_ids, test_ids = train_test_split(temp_ids, test_size=0.33, random_state=42)
    
    train_ids_set, val_ids_set, test_ids_set = set(train_ids), set(val_ids), set(test_ids)
    
    if not train_ids:
        print("é”™è¯¯ï¼šæ²¡æœ‰è½¨è¿¹è¢«åˆ†é…åˆ°è®­ç»ƒé›†ï¼Œæ— æ³•è®¡ç®—ç»Ÿè®¡æ•°æ®ã€‚")
        return False

    train_data_points = np.concatenate([all_trajectories_dict[tid] for tid in train_ids])
    means = np.mean(train_data_points, axis=0, dtype=np.float32)
    stds = np.std(train_data_points, axis=0, dtype=np.float32)
    pd.DataFrame([means]).to_csv(os.path.join(save_path, 'mean.csv'), index=False, header=None)
    pd.DataFrame([stds]).to_csv(os.path.join(save_path, 'std.csv'), index=False, header=None)
    del train_data_points

    # æ­¥éª¤ 3: åˆ›å»ºå¹¶è¡Œä»»åŠ¡ï¼Œè®©æ¯ä¸ªä»»åŠ¡è‡ªå·±ä¿å­˜
    print("\n[æ­¥éª¤ 3: å¹¶è¡Œåˆ›å»ºçª—å£å¹¶å†™å…¥ä¸´æ—¶æ–‡ä»¶...]")
    tasks = []
    for traj_id, traj_data in all_trajectories_dict.items():
        if len(traj_data) < sequence_length + prediction_steps:
            continue
        
        set_name = ''
        if traj_id in train_ids_set: set_name = 'train'
        elif traj_id in val_ids_set: set_name = 'val'
        else: set_name = 'test'
        
        tasks.append((traj_id, traj_data, sequence_length, prediction_steps, time_step, means, stds, temp_dir, set_name))
    
    # æ‰§è¡Œå¹¶è¡Œå¤„ç†
    temp_file_results = {'train': [], 'val': [], 'test': []}
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        results = list(tqdm(executor.map(create_windows_and_save_to_temp, tasks), total=len(tasks), desc="å¹¶è¡Œå¤„ç†è½¨è¿¹"))

    # æ”¶é›†è¿”å›çš„è½»é‡çº§ç»“æœ
    for res in results:
        if res:
            temp_file_results[res['set_name']].append(res)
            
    # æ­¥éª¤ 4: åˆå¹¶æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
    print("\n[æ­¥éª¤ 4: åˆå¹¶ä¸´æ—¶æ–‡ä»¶...]")
    for set_name in ['train', 'val', 'test']:
        print(f"åˆå¹¶ {set_name} æ•°æ®é›†...")
        id_list = []
        
        with open(output_files_paths[f'{set_name}_x'], 'wb') as wfd_x, open(output_files_paths[f'{set_name}_y'], 'wb') as wfd_y:
            for result in tqdm(temp_file_results[set_name], desc=f"åˆå¹¶ {set_name} æ–‡ä»¶"):
                if os.path.exists(result['x_path']):
                    with open(result['x_path'], 'rb') as rfd:
                        shutil.copyfileobj(rfd, wfd_x)
                if os.path.exists(result['y_path']):
                    with open(result['y_path'], 'rb') as rfd:
                        shutil.copyfileobj(rfd, wfd_y)
                id_list.extend([result['id']] * result['count'])
        
        pd.DataFrame({'id': id_list}).to_csv(output_files_paths[f'{set_name}_ids'], index=False)

    # æ­¥éª¤ 5: æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    print("\n[æ­¥éª¤ 5: æ¸…ç†ä¸´æ—¶æ–‡ä»¶...]")
    shutil.rmtree(temp_dir)
    
    print("\n--- âœ… å¤„ç†å®Œæˆ ---")
    return True

if __name__ == '__main__':
    seq_length = 192
    pred_steps = 72
    time_step = 1 # è®¾ç½®ä¸º1æ¥æµ‹è¯•æœ€æç«¯çš„æƒ…å†µ

    NUM_WORKERS = 64 # å»ºè®®è®¾ä¸ºç‰©ç†æ ¸å¿ƒæ•°

    save_paths = f"./dataset/input-{seq_length}_output-{pred_steps}_timestep-{time_step}_final"
    if not os.path.exists(save_paths):
        os.makedirs(save_paths)

    process_all_files_final(file_path='./dataset/advanced_cleaned_2022-05-01.csv',
                            save_path=save_paths,
                            sequence_length=seq_length,
                            prediction_steps=pred_steps,
                            time_step=time_step,
                            num_workers=NUM_WORKERS)
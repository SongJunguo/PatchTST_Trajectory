# -*- coding: utf-8 -*-
# ==============================================================================
# ** æ¨¡åž‹æŽ¨ç†ä¸»è„šæœ¬ (Webå¯è§†åŒ–ä¸“ç”¨) **
#
# ** ç‰ˆæœ¬: 1.0 **
#
# ** åŠŸèƒ½: **
#   - åŠ è½½é¢„è®­ç»ƒçš„PatchTSTæ¨¡åž‹ã€‚
#   - ä½¿ç”¨ data_loader_for_inference.py ä¸­çš„ä¸“ç”¨æ•°æ®åŠ è½½å™¨ã€‚
#   - å¯¹åŽ†å²æ•°æ®è¿›è¡Œæ»‘çª—é¢„æµ‹ã€‚
#   - å°†é¢„æµ‹ç»“æžœä¸Žå…ƒæ•°æ®(ID, anchor_time, TASK, PLANETYPE)ç»“åˆã€‚
#   - å°†æœ€ç»ˆç»“æžœä¿å­˜ä¸ºå•ä¸ªParquetæ–‡ä»¶ï¼Œä¾›Webç«¯ä½¿ç”¨ã€‚
#
# ** è¿è¡Œè¯´æ˜Ž: **
#   (é€šè¿‡ run_inference_for_web.sh è„šæœ¬è°ƒç”¨)
# ==============================================================================

import os
import torch
import numpy as np
import pandas as pd
import argparse
from tqdm import tqdm

# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æŽ¨ç†æ•°æ®åŠ è½½å™¨
from data_provider.data_loader_for_inference import Dataset_Flight_Inference
# å¯¼å…¥ä¸»è¦çš„å®žéªŒç±»ï¼Œæˆ‘ä»¬å°†ç»§æ‰¿å®ƒ
from exp.exp_main import Exp_Main

class Exp_Inference(Exp_Main):
    def __init__(self, args):
        super(Exp_Inference, self).__init__(args)

    def _get_data(self, flag):
        """
        é‡å†™ _get_data æ–¹æ³•ï¼Œä»¥ä½¿ç”¨æˆ‘ä»¬ä¸ºæŽ¨ç†åˆ›å»ºçš„ä¸“ç”¨Datasetã€‚
        """
        if flag == 'pred':
            # å¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„æŽ¨ç†æ•°æ®é›†
            data_set = Dataset_Flight_Inference(
                root_path=self.args.root_path,
                data_path=self.args.data_path,
                size=[self.args.seq_len, self.args.label_len, self.args.pred_len],
                features=self.args.features,
                target=self.args.target,
                scale=True, # æŽ¨ç†æ—¶éœ€è¦ä¸Žè®­ç»ƒæ—¶ä¸€è‡´çš„å½’ä¸€åŒ–
                timeenc=self.args.embed_type,
                freq=self.args.freq,
                stride=self.args.dataloader_stride, # ðŸš€ ä¼ é€’æ»‘çª—æ­¥é•¿
                stats_path=self.args.stats_path # ä¼ é€’å½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
            )
            # è§£æžå­—ç¬¦ä¸²å‚æ•°ä¸ºå¸ƒå°”å€¼
            pin_memory = self.args.pin_memory.lower() == 'true'
            persistent_workers = self.args.persistent_workers.lower() == 'true' and self.args.num_workers > 0

            data_loader = torch.utils.data.DataLoader(
                data_set,
                batch_size=self.args.batch_size,
                shuffle=False,
                num_workers=self.args.num_workers,
                drop_last=False,
                pin_memory=pin_memory,  # æ ¹æ®å‚æ•°å¯ç”¨å†…å­˜é”å®š
                persistent_workers=persistent_workers,  # æ ¹æ®å‚æ•°ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
                prefetch_factor=4 if self.args.num_workers > 0 else 2  # åŠ¨æ€è°ƒæ•´é¢„å–å› å­
            )
            return data_set, data_loader
        else:
            # å¯¹äºŽå…¶ä»–æƒ…å†µï¼ˆå¦‚è®­ç»ƒã€éªŒè¯ï¼‰ï¼Œæ²¿ç”¨çˆ¶ç±»çš„è¡Œä¸º
            return super()._get_data(flag)

    def predict(self, setting, load=True):
        """
        é‡å†™ predict æ–¹æ³•ï¼Œä»¥å®žçŽ°æˆ‘ä»¬çš„æ ¸å¿ƒæŽ¨ç†å’Œä¿å­˜é€»è¾‘ã€‚
        """
        pred_data, pred_loader = self._get_data(flag='pred')

        if load:
            path = os.path.join(self.args.checkpoints, setting)
            best_model_path = os.path.join(path, 'checkpoint.pth')
            print(f"æ­£åœ¨ä»Ž {best_model_path} åŠ è½½æ¨¡åž‹...")

            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(best_model_path, map_location=self.device)

            # æ£€æŸ¥æ£€æŸ¥ç‚¹æ ¼å¼
            if 'model_state_dict' in checkpoint:
                # å®Œæ•´çš„è®­ç»ƒæ£€æŸ¥ç‚¹æ ¼å¼
                state_dict = checkpoint['model_state_dict']
                print(f"ä»Žè®­ç»ƒæ£€æŸ¥ç‚¹åŠ è½½æ¨¡åž‹ (epoch: {checkpoint.get('epoch', 'unknown')})")
            else:
                # ä»…æ¨¡åž‹çŠ¶æ€å­—å…¸æ ¼å¼
                state_dict = checkpoint
                print("ä»Žæ¨¡åž‹çŠ¶æ€å­—å…¸åŠ è½½æ¨¡åž‹")

            # å¤„ç† DataParallel æ¨¡åž‹çš„é”®åé—®é¢˜
            if any(key.startswith('module.') for key in state_dict.keys()):
                # ç§»é™¤ 'module.' å‰ç¼€
                new_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key[7:] if key.startswith('module.') else key  # ç§»é™¤ 'module.' å‰ç¼€
                    new_state_dict[new_key] = value
                state_dict = new_state_dict
                print("æ£€æµ‹åˆ° DataParallel æ¨¡åž‹ï¼Œå·²ç§»é™¤ 'module.' å‰ç¼€")

            self.model.load_state_dict(state_dict)

        results_list = []
        self.model.eval()

        # ðŸš€ æ€§èƒ½ç›‘æŽ§
        import time
        total_batches = len(pred_loader)
        print(f"å¼€å§‹æŽ¨ç†ï¼Œå…± {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ‰¹æ¬¡å¤§å°: {self.args.batch_size}")

        overall_start = time.time()
        batch_times = []

        with torch.no_grad():
            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, meta_info) in enumerate(tqdm(pred_loader, desc="è¿›è¡Œé¢„æµ‹")):
                batch_start = time.time()
                # ä½¿ç”¨éžé˜»å¡žä¼ è¾“ï¼Œæé«˜GPUåˆ©ç”¨çŽ‡
                batch_x = batch_x.float().to(self.device, non_blocking=True)
                batch_x_mark = batch_x_mark.float().to(self.device, non_blocking=True)

                # PatchTSTæ¨¡åž‹åªéœ€è¦è¾“å…¥åºåˆ—
                outputs = self.model(batch_x)

                # åå½’ä¸€åŒ– - å¤„ç†3Dæ•°ç»„
                outputs_np = outputs.detach().cpu().numpy()  # [batch_size, pred_len, num_features]
                batch_size, pred_len, num_features = outputs_np.shape

                # é‡å¡‘ä¸º2Dè¿›è¡Œåå½’ä¸€åŒ–
                outputs_2d = outputs_np.reshape(-1, num_features)  # [batch_size * pred_len, num_features]
                outputs_2d = pred_data.inverse_transform(outputs_2d)

                # é‡å¡‘å›ž3D
                outputs = outputs_2d.reshape(batch_size, pred_len, num_features)
                
                # ðŸš€ ä¼˜åŒ–ï¼šæ‰¹é‡å¤„ç†ï¼Œé¿å…é€ä¸ªåˆ›å»ºDataFrame
                batch_size = outputs.shape[0]

                # æ‰¹é‡é‡å¡‘é¢„æµ‹ç»“æžœï¼š[batch_size, pred_len, num_features] -> [batch_size * pred_len, num_features]
                outputs_flat = outputs.reshape(-1, outputs.shape[-1])  # [batch_size * pred_len, 3]

                # æ‰¹é‡åˆ›å»ºåŸºç¡€æ•°æ®
                batch_results = []
                for j in range(batch_size):
                    # æå–å½“å‰æ ·æœ¬çš„é¢„æµ‹ç»“æžœ
                    start_idx = j * pred_len
                    end_idx = start_idx + pred_len
                    sample_preds = outputs_flat[start_idx:end_idx]

                    # åˆ›å»ºå½“å‰æ ·æœ¬çš„ç»“æžœå­—å…¸ï¼ˆé¿å…DataFrameåˆ›å»ºï¼‰
                    sample_result = {
                        'H_predicted': sample_preds[:, 0],
                        'JD_predicted': sample_preds[:, 1],
                        'WD_predicted': sample_preds[:, 2],
                        'Pred_trajectory_id': [meta_info['Pred_trajectory_id'][j]] * pred_len,
                        'prediction_anchor_time': [meta_info['prediction_anchor_time'][j]] * pred_len,
                        'TASK': [meta_info['TASK'][j]] * pred_len,
                        'PLANETYPE': [meta_info['PLANETYPE'][j]] * pred_len
                    }
                    batch_results.append(sample_result)

                # æ‰¹é‡åˆ›å»ºDataFrameï¼ˆä¸€æ¬¡æ€§æ“ä½œï¼‰
                if batch_results:
                    # åˆå¹¶æ‰€æœ‰æ ·æœ¬çš„æ•°æ®
                    combined_data = {
                        'H_predicted': np.concatenate([r['H_predicted'] for r in batch_results]),
                        'JD_predicted': np.concatenate([r['JD_predicted'] for r in batch_results]),
                        'WD_predicted': np.concatenate([r['WD_predicted'] for r in batch_results]),
                        'Pred_trajectory_id': [item for r in batch_results for item in r['Pred_trajectory_id']],
                        'prediction_anchor_time': [item for r in batch_results for item in r['prediction_anchor_time']],
                        'TASK': [item for r in batch_results for item in r['TASK']],
                        'PLANETYPE': [item for r in batch_results for item in r['PLANETYPE']]
                    }

                    # ä¸€æ¬¡æ€§åˆ›å»ºDataFrame
                    batch_df = pd.DataFrame(combined_data)
                    results_list.append(batch_df)

                # è®°å½•æ‰¹æ¬¡å¤„ç†æ—¶é—´
                batch_end = time.time()
                batch_time = batch_end - batch_start
                batch_times.append(batch_time)

                # æ¯10ä¸ªæ‰¹æ¬¡è¾“å‡ºä¸€æ¬¡æ€§èƒ½ç»Ÿè®¡
                if (i + 1) % 10 == 0:
                    avg_batch_time = sum(batch_times[-10:]) / min(10, len(batch_times))
                    print(f"æ‰¹æ¬¡ {i+1}/{total_batches}, å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.3f}s")

        # è¾“å‡ºæ€»ä½“æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - overall_start
        avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0
        print(f"\nðŸš€ æ€§èƒ½ç»Ÿè®¡:")
        print(f"  æ€»æŽ¨ç†æ—¶é—´: {total_time:.2f}s")
        print(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.3f}s")
        print(f"  åžåé‡: {total_batches/total_time:.2f} batches/s")
        print(f"  æ ·æœ¬åžåé‡: {total_batches*self.args.batch_size/total_time:.2f} samples/s")

        if not results_list:
            print("è­¦å‘Šï¼šæ²¡æœ‰ç”Ÿæˆä»»ä½•é¢„æµ‹ç»“æžœã€‚")
            return

        # åˆå¹¶æ‰€æœ‰ç»“æžœå¹¶ä¿å­˜
        final_results_df = pd.concat(results_list, ignore_index=True)
        
        # è°ƒæ•´åˆ—é¡ºåº
        final_results_df = final_results_df[[
            'Pred_trajectory_id', 
            'prediction_anchor_time', 
            'H_predicted', 
            'JD_predicted', 
            'WD_predicted', 
            'TASK', 
            'PLANETYPE'
        ]]

        # ç»“æžœä¿å­˜
        folder_path = './results/' + setting + '/'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
        
        output_path = os.path.join(folder_path, 'prediction_results.parquet')
        final_results_df.to_parquet(output_path, index=False, compression='zstd')
        
        print(f"é¢„æµ‹å®Œæˆï¼ç»“æžœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"æ€»å…±ä¸º {final_results_df['Pred_trajectory_id'].nunique()} æ¡è½¨è¿¹ç”Ÿæˆäº† {len(final_results_df)} ä¸ªé¢„æµ‹ç‚¹ã€‚")

        return

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PatchTST for Time Series Forecasting - Inference for Web')

    # ä»Ž run_longExp.py å¤åˆ¶æ‰€æœ‰å‚æ•°
    parser.add_argument('--is_training', type=int, default=0, help='status')
    parser.add_argument('--model_id', type=str, required=True, help='model id')
    parser.add_argument('--model', type=str, default='PatchTST', help='model name')
    parser.add_argument('--data', type=str, default='flight', help='dataset type')
    parser.add_argument('--root_path', type=str, default='./PatchTST_supervised/dataset/processed_for_web/', help='root path of the data file')
    parser.add_argument('--data_path', type=str, default='final_processed_trajectories.parquet', help='data file')
    parser.add_argument('--features', type=str, default='M', help='M, S, MS')
    parser.add_argument('--target', type=str, default='H', help='target feature')
    parser.add_argument('--freq', type=str, default='s', help='freq for time features encoding')
    parser.add_argument('--checkpoints', type=str, default='./PatchTST_supervised/checkpoints/', help='location of model checkpoints')
    parser.add_argument('--seq_len', type=int, default=192, help='input sequence length')
    parser.add_argument('--label_len', type=int, default=0, help='start token length')
    parser.add_argument('--pred_len', type=int, default=72, help='prediction sequence length')
    parser.add_argument('--enc_in', type=int, default=3, help='encoder input size')
    parser.add_argument('--c_out', type=int, default=3, help='output size')
    parser.add_argument('--d_model', type=int, default=512, help='dimension of model')
    parser.add_argument('--n_heads', type=int, default=32, help='num of heads')
    parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')
    parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')
    parser.add_argument('--d_ff', type=int, default=1024, help='dimension of fcn')
    parser.add_argument('--dropout', type=float, default=0.2, help='dropout')
    parser.add_argument('--fc_dropout', type=float, default=0.2, help='fully connected dropout')
    parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')
    parser.add_argument('--patch_len', type=int, default=16, help='patch length')
    parser.add_argument('--stride', type=int, default=8, help='stride')
    parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')
    parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')
    parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')
    parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')
    parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')
    parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')
    parser.add_argument('--individual', type=int, default=1, help='individual head; True 1 False 0')
    parser.add_argument('--embed_type', type=int, default=1, help='0: default 1: value embedding + temporal embedding + positional embedding')
    parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')
    parser.add_argument('--batch_size', type=int, default=1024, help='batch size of train input data')
    parser.add_argument('--pin_memory', type=str, default='true', help='enable pin memory for faster GPU transfer')
    parser.add_argument('--persistent_workers', type=str, default='true', help='keep worker processes alive')
    parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')
    parser.add_argument('--gpu', type=int, default=0, help='gpu')
    parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)
    parser.add_argument('--devices', type=str, default='0,1', help='device ids of multile gpus')
    parser.add_argument('--des', type=str, default='Exp', help='exp description')
    # ðŸš€ æ–°å¢žï¼šæ•°æ®åŠ è½½å™¨æ»‘çª—æ­¥é•¿å‚æ•°
    parser.add_argument('--dataloader_stride', type=int, default=1, help='stride for dataloader sliding window')
    # ðŸš€ æ–°å¢žï¼šå½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
    parser.add_argument('--stats_path', type=str, required=True, help='path to the normalization stats file')

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False
    if args.use_gpu and args.use_multi_gpu:
        args.devices = args.devices.replace(' ', '')
        device_ids = args.devices.split(',')
        args.device_ids = [int(id_) for id_ in device_ids]
        args.gpu = args.device_ids[0]

    print('Args in experiment:')
    print(args)

    # åˆå§‹åŒ–å¹¶è¿è¡ŒæŽ¨ç†å®žéªŒ
    exp = Exp_Inference(args)
    # ä½¿ç”¨ä¸Žè®­ç»ƒæ—¶ç›¸åŒçš„ setting æ ¼å¼
    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(
        args.model_id,
        args.model,
        args.data,
        args.features,
        args.seq_len,
        args.label_len,
        args.pred_len,
        args.d_model,
        args.n_heads,
        args.e_layers,
        args.d_layers,
        args.d_ff,
        1,  # factor (å›ºå®šå€¼)
        'timeF',  # embed (å›ºå®šå€¼)
        True,  # distil (å›ºå®šå€¼)
        args.des,
        0  # iteration (å›ºå®šå€¼)
    )
    
    print(f'>>>>>>>start predicting : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>')
    exp.predict(setting, load=True)

    torch.cuda.empty_cache()
# -*- coding: utf-8 -*-
# ==============================================================================
# ** æ¨¡åž‹æŽ¨ç†ä¸»è„šæœ¬ (Webå¯è§†åŒ–ä¸“ç”¨) **
#
# ** ç‰ˆæœ¬: 1.0 **
#
# ** åŠŸèƒ½: **
#   - åŠ è½½é¢„è®­ç»ƒçš„PatchTSTæ¨¡åž‹ã€‚
#   - ä½¿ç”¨ data_loader_for_inference.py ä¸­çš„ä¸“ç”¨æ•°æ®åŠ è½½å™¨ã€‚
#   - å¯¹åŽ†å²æ•°æ®è¿›è¡Œæ»‘çª—é¢„æµ‹ã€‚
#   - å°†é¢„æµ‹ç»“æžœä¸Žå…ƒæ•°æ®(ID, anchor_time, TASK, PLANETYPE)ç»“åˆã€‚
#   - å°†æœ€ç»ˆç»“æžœä¿å­˜ä¸ºå•ä¸ªParquetæ–‡ä»¶ï¼Œä¾›Webç«¯ä½¿ç”¨ã€‚
#
# ** è¿è¡Œè¯´æ˜Ž: **
#   (é€šè¿‡ run_inference_for_web.sh è„šæœ¬è°ƒç”¨)
# ==============================================================================

import os
import torch
import numpy as np
import pandas as pd
import argparse
from tqdm import tqdm
import pyarrow as pa
import pyarrow.parquet as pq

# å¯¼å…¥æˆ‘ä»¬è‡ªå®šä¹‰çš„æŽ¨ç†æ•°æ®åŠ è½½å™¨
from data_provider.data_loader_for_inference import Dataset_Flight_Inference
# å¯¼å…¥ä¸»è¦çš„å®žéªŒç±»ï¼Œæˆ‘ä»¬å°†ç»§æ‰¿å®ƒ
from exp.exp_main import Exp_Main

class Exp_Inference(Exp_Main):
    def __init__(self, args):
        super(Exp_Inference, self).__init__(args)

    def _get_data(self, flag):
        """
        é‡å†™ _get_data æ–¹æ³•ï¼Œä»¥ä½¿ç”¨æˆ‘ä»¬ä¸ºæŽ¨ç†åˆ›å»ºçš„ä¸“ç”¨Datasetã€‚
        """
        if flag == 'pred':
            # å¼ºåˆ¶ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„æŽ¨ç†æ•°æ®é›†
            data_set = Dataset_Flight_Inference(
                root_path=self.args.root_path,
                data_path=self.args.data_path,
                size=[self.args.seq_len, self.args.label_len, self.args.pred_len],
                features=self.args.features,
                target=self.args.target,
                scale=True, # æŽ¨ç†æ—¶éœ€è¦ä¸Žè®­ç»ƒæ—¶ä¸€è‡´çš„å½’ä¸€åŒ–
                timeenc=self.args.embed_type,
                freq=self.args.freq,
                stride=self.args.dataloader_stride, # ðŸš€ ä¼ é€’æ»‘çª—æ­¥é•¿
                stats_path=self.args.stats_path # ä¼ é€’å½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
            )
            # è§£æžå­—ç¬¦ä¸²å‚æ•°ä¸ºå¸ƒå°”å€¼
            pin_memory = self.args.pin_memory.lower() == 'true'
            persistent_workers = self.args.persistent_workers.lower() == 'true' and self.args.num_workers > 0

            data_loader = torch.utils.data.DataLoader(
                data_set,
                batch_size=self.args.batch_size,
                shuffle=False,
                num_workers=self.args.num_workers,
                drop_last=False,
                pin_memory=pin_memory,  # æ ¹æ®å‚æ•°å¯ç”¨å†…å­˜é”å®š
                persistent_workers=persistent_workers,  # æ ¹æ®å‚æ•°ä¿æŒå·¥ä½œè¿›ç¨‹æ´»è·ƒ
                prefetch_factor=4 if self.args.num_workers > 0 else 2  # åŠ¨æ€è°ƒæ•´é¢„å–å› å­
            )
            return data_set, data_loader
        else:
            # å¯¹äºŽå…¶ä»–æƒ…å†µï¼ˆå¦‚è®­ç»ƒã€éªŒè¯ï¼‰ï¼Œæ²¿ç”¨çˆ¶ç±»çš„è¡Œä¸º
            return super()._get_data(flag)

    def predict(self, setting, load=True):
        """
        é‡å†™ predict æ–¹æ³•ï¼Œä»¥å®žçŽ°æˆ‘ä»¬çš„æ ¸å¿ƒæŽ¨ç†å’Œä¿å­˜é€»è¾‘ã€‚
        """
        pred_data, pred_loader = self._get_data(flag='pred')

        if load:
            path = os.path.join(self.args.checkpoints, setting)
            best_model_path = os.path.join(path, 'checkpoint.pth')
            print(f"æ­£åœ¨ä»Ž {best_model_path} åŠ è½½æ¨¡åž‹...")

            # åŠ è½½æ£€æŸ¥ç‚¹
            checkpoint = torch.load(best_model_path, map_location=self.device)

            # æ£€æŸ¥æ£€æŸ¥ç‚¹æ ¼å¼
            if 'model_state_dict' in checkpoint:
                # å®Œæ•´çš„è®­ç»ƒæ£€æŸ¥ç‚¹æ ¼å¼
                state_dict = checkpoint['model_state_dict']
                print(f"ä»Žè®­ç»ƒæ£€æŸ¥ç‚¹åŠ è½½æ¨¡åž‹ (epoch: {checkpoint.get('epoch', 'unknown')})")
            else:
                # ä»…æ¨¡åž‹çŠ¶æ€å­—å…¸æ ¼å¼
                state_dict = checkpoint
                print("ä»Žæ¨¡åž‹çŠ¶æ€å­—å…¸åŠ è½½æ¨¡åž‹")

            # å¤„ç† DataParallel æ¨¡åž‹çš„é”®åé—®é¢˜
            if any(key.startswith('module.') for key in state_dict.keys()):
                # ç§»é™¤ 'module.' å‰ç¼€
                new_state_dict = {}
                for key, value in state_dict.items():
                    new_key = key[7:] if key.startswith('module.') else key  # ç§»é™¤ 'module.' å‰ç¼€
                    new_state_dict[new_key] = value
                state_dict = new_state_dict
                print("æ£€æµ‹åˆ° DataParallel æ¨¡åž‹ï¼Œå·²ç§»é™¤ 'module.' å‰ç¼€")

            self.model.load_state_dict(state_dict)

        self.model.eval()

        # ç»“æžœä¿å­˜è·¯å¾„
        folder_path = './results/' + setting + '/'
        if not os.path.exists(folder_path):
            os.makedirs(folder_path)
        output_path = os.path.join(folder_path, 'prediction_results.parquet')

        # å®šä¹‰Parquetæ–‡ä»¶çš„Schema
        schema = pa.schema([
            pa.field('Pred_trajectory_id', pa.string()),
            pa.field('prediction_anchor_time', pa.timestamp('ms')),
            pa.field('H_predicted', pa.float32()),
            pa.field('JD_predicted', pa.float32()),
            pa.field('WD_predicted', pa.float32())
        ])

        # åˆå§‹åŒ–Parquetå†™å…¥å™¨
        writer = pq.ParquetWriter(output_path, schema, compression='zstd', version='1.0')

        total_rows_written = 0
        processed_ids = set()

        # æ€§èƒ½ç›‘æŽ§
        import time
        total_batches = len(pred_loader)
        print(f"å¼€å§‹æŽ¨ç†ï¼Œå…± {total_batches} ä¸ªæ‰¹æ¬¡ï¼Œæ‰¹æ¬¡å¤§å°: {self.args.batch_size}")
        overall_start = time.time()
        batch_times = []

        try:
            with torch.no_grad():
                for i, (batch_x, batch_y, batch_x_mark, batch_y_mark, meta_info) in enumerate(tqdm(pred_loader, desc="è¿›è¡Œé¢„æµ‹")):
                    batch_start = time.time()
                    
                    batch_x = batch_x.float().to(self.device, non_blocking=True)
                    batch_x_mark = batch_x_mark.float().to(self.device, non_blocking=True)

                    outputs = self.model(batch_x)
                    outputs_np = outputs.detach().cpu().numpy()
                    batch_size, pred_len, num_features = outputs_np.shape

                    if batch_size == 0:
                        continue

                    outputs_2d = outputs_np.reshape(-1, num_features)
                    outputs_2d = pred_data.inverse_transform(outputs_2d)
                    
                    # æ‰©å±•å…ƒæ•°æ®
                    ids = np.repeat(meta_info['Pred_trajectory_id'], pred_len)
                    timestamps = np.repeat(meta_info['prediction_anchor_time'], pred_len)

                    # åˆ›å»ºå½“å‰æ‰¹æ¬¡çš„DataFrame
                    batch_df = pd.DataFrame({
                        'Pred_trajectory_id': ids,
                        'prediction_anchor_time': pd.to_datetime(timestamps),
                        'H_predicted': outputs_2d[:, 0].astype('float32'),
                        'JD_predicted': outputs_2d[:, 1].astype('float32'),
                        'WD_predicted': outputs_2d[:, 2].astype('float32')
                    })

                    # ç›´æŽ¥å†™å…¥æ–‡ä»¶
                    table = pa.Table.from_pandas(batch_df, schema=schema, preserve_index=False)
                    writer.write_table(table)

                    total_rows_written += len(batch_df)
                    processed_ids.update(meta_info['Pred_trajectory_id'])

                    batch_end = time.time()
                    batch_times.append(batch_end - batch_start)

                    if (i + 1) % 10 == 0:
                        avg_batch_time = sum(batch_times[-10:]) / min(10, len(batch_times))
                        print(f"æ‰¹æ¬¡ {i+1}/{total_batches}, å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.3f}s")

            total_time = time.time() - overall_start
            avg_batch_time = sum(batch_times) / len(batch_times) if batch_times else 0
            print(f"\nðŸš€ æ€§èƒ½ç»Ÿè®¡:")
            print(f"  æ€»æŽ¨ç†æ—¶é—´: {total_time:.2f}s")
            print(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.3f}s")
            print(f"  åžåé‡: {total_batches/total_time:.2f} batches/s")
            print(f"  æ ·æœ¬åžåé‡: {total_batches*self.args.batch_size/total_time:.2f} samples/s")

        finally:
            # ç¡®ä¿æ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œéƒ½å…³é—­å†™å…¥å™¨
            print("æŽ¨ç†å¾ªçŽ¯ç»“æŸï¼Œæ­£åœ¨å…³é—­æ–‡ä»¶å†™å…¥å™¨...")
            writer.close()
            print("å†™å…¥å®Œæˆã€‚")

        print(f"é¢„æµ‹å®Œæˆï¼ç»“æžœå·²ä¿å­˜åˆ°: {output_path}")
        print(f"æ€»å…±ä¸º {len(processed_ids)} æ¡è½¨è¿¹ç”Ÿæˆäº† {total_rows_written} ä¸ªé¢„æµ‹ç‚¹ã€‚")

        return

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='PatchTST for Time Series Forecasting - Inference for Web')

    # ä»Ž run_longExp.py å¤åˆ¶æ‰€æœ‰å‚æ•°
    parser.add_argument('--is_training', type=int, default=0, help='status')
    parser.add_argument('--model_id', type=str, required=True, help='model id')
    parser.add_argument('--model', type=str, default='PatchTST', help='model name')
    parser.add_argument('--data', type=str, default='flight', help='dataset type')
    parser.add_argument('--root_path', type=str, default='./PatchTST_supervised/dataset/processed_for_web/', help='root path of the data file')
    parser.add_argument('--data_path', type=str, default='final_processed_trajectories.parquet', help='data file')
    parser.add_argument('--features', type=str, default='M', help='M, S, MS')
    parser.add_argument('--target', type=str, default='H', help='target feature')
    parser.add_argument('--freq', type=str, default='s', help='freq for time features encoding')
    parser.add_argument('--checkpoints', type=str, default='./PatchTST_supervised/checkpoints/', help='location of model checkpoints')
    parser.add_argument('--seq_len', type=int, default=192, help='input sequence length')
    parser.add_argument('--label_len', type=int, default=0, help='start token length')
    parser.add_argument('--pred_len', type=int, default=72, help='prediction sequence length')
    parser.add_argument('--enc_in', type=int, default=3, help='encoder input size')
    parser.add_argument('--c_out', type=int, default=3, help='output size')
    parser.add_argument('--d_model', type=int, default=512, help='dimension of model')
    parser.add_argument('--n_heads', type=int, default=32, help='num of heads')
    parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')
    parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')
    parser.add_argument('--d_ff', type=int, default=1024, help='dimension of fcn')
    parser.add_argument('--dropout', type=float, default=0.2, help='dropout')
    parser.add_argument('--fc_dropout', type=float, default=0.2, help='fully connected dropout')
    parser.add_argument('--head_dropout', type=float, default=0.0, help='head dropout')
    parser.add_argument('--patch_len', type=int, default=16, help='patch length')
    parser.add_argument('--stride', type=int, default=8, help='stride')
    parser.add_argument('--padding_patch', default='end', help='None: None; end: padding on the end')
    parser.add_argument('--revin', type=int, default=1, help='RevIN; True 1 False 0')
    parser.add_argument('--affine', type=int, default=0, help='RevIN-affine; True 1 False 0')
    parser.add_argument('--subtract_last', type=int, default=0, help='0: subtract mean; 1: subtract last')
    parser.add_argument('--decomposition', type=int, default=0, help='decomposition; True 1 False 0')
    parser.add_argument('--kernel_size', type=int, default=25, help='decomposition-kernel')
    parser.add_argument('--individual', type=int, default=1, help='individual head; True 1 False 0')
    parser.add_argument('--embed_type', type=int, default=1, help='0: default 1: value embedding + temporal embedding + positional embedding')
    parser.add_argument('--embed', type=str, default='timeF', help='time features encoding, options:[timeF, fixed, learned]')
    parser.add_argument('--num_workers', type=int, default=10, help='data loader num workers')
    parser.add_argument('--batch_size', type=int, default=1024, help='batch size of train input data')
    parser.add_argument('--pin_memory', type=str, default='true', help='enable pin memory for faster GPU transfer')
    parser.add_argument('--persistent_workers', type=str, default='true', help='keep worker processes alive')
    parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')
    parser.add_argument('--gpu', type=int, default=0, help='gpu')
    parser.add_argument('--use_multi_gpu', action='store_true', help='use multiple gpus', default=False)
    parser.add_argument('--devices', type=str, default='0,1', help='device ids of multile gpus')
    parser.add_argument('--des', type=str, default='Exp', help='exp description')
    # ðŸš€ æ–°å¢žï¼šæ•°æ®åŠ è½½å™¨æ»‘çª—æ­¥é•¿å‚æ•°
    parser.add_argument('--dataloader_stride', type=int, default=1, help='stride for dataloader sliding window')
    # ðŸš€ æ–°å¢žï¼šå½’ä¸€åŒ–ç»Ÿè®¡æ–‡ä»¶è·¯å¾„
    parser.add_argument('--stats_path', type=str, required=True, help='path to the normalization stats file')

    args = parser.parse_args()

    # è®¾ç½®è®¾å¤‡
    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False
    if args.use_gpu and args.use_multi_gpu:
        args.devices = args.devices.replace(' ', '')
        device_ids = args.devices.split(',')
        args.device_ids = [int(id_) for id_ in device_ids]
        args.gpu = args.device_ids[0]

    print('Args in experiment:')
    print(args)

    # åˆå§‹åŒ–å¹¶è¿è¡ŒæŽ¨ç†å®žéªŒ
    exp = Exp_Inference(args)
    # ä½¿ç”¨ä¸Žè®­ç»ƒæ—¶ç›¸åŒçš„ setting æ ¼å¼
    setting = '{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}'.format(
        args.model_id,
        args.model,
        args.data,
        args.features,
        args.seq_len,
        args.label_len,
        args.pred_len,
        args.d_model,
        args.n_heads,
        args.e_layers,
        args.d_layers,
        args.d_ff,
        1,  # factor (å›ºå®šå€¼)
        args.embed,  # embed (å›ºå®šå€¼)
        True,  # distil (å›ºå®šå€¼)
        args.des,
        0  # iteration (å›ºå®šå€¼)
    )
    
    print(f'>>>>>>>start predicting : {setting}>>>>>>>>>>>>>>>>>>>>>>>>>>')
    exp.predict(setting, load=True)

    torch.cuda.empty_cache()
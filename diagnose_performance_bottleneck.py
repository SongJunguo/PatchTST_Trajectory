#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ€§èƒ½ç“¶é¢ˆè¯Šæ–­è„šæœ¬
ä¸“é—¨åˆ†ææ¨ç†è¿‡ç¨‹ä¸­çš„æ€§èƒ½ç“¶é¢ˆ
"""

import os
import sys
import time
import psutil
import threading
import subprocess
from datetime import datetime
import numpy as np

def profile_dataloader_performance():
    """åˆ†ææ•°æ®åŠ è½½å™¨æ€§èƒ½"""
    print("=== æ•°æ®åŠ è½½å™¨æ€§èƒ½åˆ†æ ===")
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    sys.path.append('./PatchTST_supervised')
    from data_provider.data_loader_for_inference import Dataset_Flight_Inference
    import torch
    from torch.utils.data import DataLoader
    
    # æµ‹è¯•ä¸åŒé…ç½®
    configs = [
        {"num_workers": 0, "batch_size": 512, "pin_memory": False},
        {"num_workers": 1, "batch_size": 512, "pin_memory": True},
        {"num_workers": 2, "batch_size": 512, "pin_memory": True},
        {"num_workers": 4, "batch_size": 512, "pin_memory": True},
        {"num_workers": 4, "batch_size": 1024, "pin_memory": True},
        {"num_workers": 4, "batch_size": 2048, "pin_memory": True},
    ]
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    data_path = "./PatchTST_supervised/dataset/processed_for_web/history_data.parquet"
    if not os.path.exists(data_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {data_path}")
        return
    
    print(f"ä½¿ç”¨æ•°æ®æ–‡ä»¶: {data_path}")
    
    for config in configs:
        print(f"\n--- æµ‹è¯•é…ç½®: {config} ---")
        
        try:
            # åˆ›å»ºæ•°æ®é›†
            dataset_start = time.time()
            dataset = Dataset_Flight_Inference(
                root_path="./PatchTST_supervised/dataset/processed_for_web/",
                data_path="history_data.parquet",
                size=[192, 0, 72],
                features='M',
                target='H',
                scale=True,
                timeenc=1,
                freq='s'
            )
            dataset_time = time.time() - dataset_start
            print(f"æ•°æ®é›†åˆ›å»ºæ—¶é—´: {dataset_time:.2f}s")
            print(f"æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
            
            # åˆ›å»ºæ•°æ®åŠ è½½å™¨
            dataloader = DataLoader(
                dataset,
                batch_size=config['batch_size'],
                shuffle=False,
                num_workers=config['num_workers'],
                drop_last=False,
                pin_memory=config['pin_memory'],
                persistent_workers=config['num_workers'] > 0,
                prefetch_factor=4 if config['num_workers'] > 0 else 2
            )
            
            # æµ‹è¯•æ•°æ®åŠ è½½æ€§èƒ½
            print(f"å¼€å§‹æµ‹è¯•æ•°æ®åŠ è½½ï¼Œå…± {len(dataloader)} ä¸ªæ‰¹æ¬¡...")
            
            load_times = []
            start_time = time.time()
            
            for i, batch in enumerate(dataloader):
                batch_start = time.time()
                
                # æ¨¡æ‹Ÿæ•°æ®ä¼ è¾“åˆ°GPU
                if torch.cuda.is_available():
                    batch_x, batch_y, batch_x_mark, batch_y_mark, meta_info = batch
                    batch_x = batch_x.float().cuda(non_blocking=True)
                    batch_x_mark = batch_x_mark.float().cuda(non_blocking=True)
                    torch.cuda.synchronize()  # ç¡®ä¿ä¼ è¾“å®Œæˆ
                
                batch_time = time.time() - batch_start
                load_times.append(batch_time)
                
                # åªæµ‹è¯•å‰20ä¸ªæ‰¹æ¬¡
                if i >= 19:
                    break
            
            total_time = time.time() - start_time
            avg_batch_time = np.mean(load_times)
            
            print(f"ç»“æœ:")
            print(f"  æ€»æ—¶é—´: {total_time:.2f}s")
            print(f"  å¹³å‡æ‰¹æ¬¡æ—¶é—´: {avg_batch_time:.3f}s")
            print(f"  ååé‡: {len(load_times)/total_time:.2f} batches/s")
            print(f"  æ ·æœ¬ååé‡: {len(load_times)*config['batch_size']/total_time:.2f} samples/s")
            
        except Exception as e:
            print(f"é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        
        time.sleep(1)  # è®©ç³»ç»Ÿä¼‘æ¯ä¸€ä¸‹

def profile_model_inference():
    """åˆ†ææ¨¡å‹æ¨ç†æ€§èƒ½"""
    print("\n=== æ¨¡å‹æ¨ç†æ€§èƒ½åˆ†æ ===")
    
    try:
        import torch
        import torch.nn as nn
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¨¡å‹æ¥æµ‹è¯•
        class SimpleModel(nn.Module):
            def __init__(self):
                super().__init__()
                self.linear = nn.Linear(192*3, 72*3)
                
            def forward(self, x):
                batch_size = x.shape[0]
                x = x.view(batch_size, -1)
                x = self.linear(x)
                return x.view(batch_size, 72, 3)
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = SimpleModel().to(device)
        model.eval()
        
        # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°çš„æ¨ç†æ€§èƒ½
        batch_sizes = [256, 512, 1024, 2048]
        
        for batch_size in batch_sizes:
            print(f"\n--- æµ‹è¯•æ‰¹æ¬¡å¤§å°: {batch_size} ---")
            
            # åˆ›å»ºéšæœºè¾“å…¥
            x = torch.randn(batch_size, 192, 3).to(device)
            
            # é¢„çƒ­
            with torch.no_grad():
                for _ in range(5):
                    _ = model(x)
            
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            
            # æµ‹è¯•æ¨ç†æ—¶é—´
            inference_times = []
            
            with torch.no_grad():
                for _ in range(20):
                    start_time = time.time()
                    output = model(x)
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                    inference_time = time.time() - start_time
                    inference_times.append(inference_time)
            
            avg_inference_time = np.mean(inference_times)
            print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_inference_time:.4f}s")
            print(f"ååé‡: {batch_size/avg_inference_time:.2f} samples/s")
            
    except Exception as e:
        print(f"æ¨¡å‹æ¨ç†æµ‹è¯•å¤±è´¥: {e}")

def monitor_system_during_inference():
    """åœ¨æ¨ç†è¿‡ç¨‹ä¸­ç›‘æ§ç³»ç»Ÿèµ„æº"""
    print("\n=== ç³»ç»Ÿèµ„æºç›‘æ§ ===")
    
    def monitor_resources(duration=30):
        """ç›‘æ§ç³»ç»Ÿèµ„æº"""
        cpu_usage = []
        memory_usage = []
        process_count = []
        
        start_time = time.time()
        while time.time() - start_time < duration:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=None, percpu=True)
            active_cores = sum(1 for usage in cpu_percent if usage > 5.0)
            
            # å†…å­˜ä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            
            # Pythonè¿›ç¨‹æ•°
            python_processes = 0
            for proc in psutil.process_iter(['name']):
                try:
                    if 'python' in proc.info['name'].lower():
                        python_processes += 1
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            cpu_usage.append({
                'avg': np.mean(cpu_percent),
                'max': np.max(cpu_percent),
                'active_cores': active_cores
            })
            memory_usage.append(memory.percent)
            process_count.append(python_processes)
            
            time.sleep(1)
        
        return {
            'cpu': cpu_usage,
            'memory': memory_usage,
            'processes': process_count
        }
    
    # å¯åŠ¨ç›‘æ§
    print("å¼€å§‹30ç§’ç³»ç»Ÿç›‘æ§...")
    data = monitor_resources(30)
    
    # åˆ†æç»“æœ
    avg_cpu = np.mean([d['avg'] for d in data['cpu']])
    max_cpu = np.max([d['max'] for d in data['cpu']])
    avg_active_cores = np.mean([d['active_cores'] for d in data['cpu']])
    avg_memory = np.mean(data['memory'])
    avg_processes = np.mean(data['processes'])
    
    print(f"\nç›‘æ§ç»“æœ:")
    print(f"  å¹³å‡CPUä½¿ç”¨ç‡: {avg_cpu:.1f}%")
    print(f"  æœ€å¤§CPUä½¿ç”¨ç‡: {max_cpu:.1f}%")
    print(f"  å¹³å‡æ´»è·ƒæ ¸å¿ƒ: {avg_active_cores:.1f}/{psutil.cpu_count()}")
    print(f"  å¹³å‡å†…å­˜ä½¿ç”¨ç‡: {avg_memory:.1f}%")
    print(f"  å¹³å‡Pythonè¿›ç¨‹æ•°: {avg_processes:.1f}")
    
    # è¯Šæ–­å»ºè®®
    print(f"\nè¯Šæ–­å»ºè®®:")
    if avg_cpu < 20:
        print("  âš ï¸  CPUä½¿ç”¨ç‡è¿‡ä½ï¼Œå¯èƒ½å­˜åœ¨I/Oç“¶é¢ˆæˆ–è¿›ç¨‹ç­‰å¾…")
    if avg_active_cores < psutil.cpu_count() * 0.3:
        print("  âš ï¸  æ´»è·ƒCPUæ ¸å¿ƒè¿‡å°‘ï¼Œå¤šçº¿ç¨‹æ•ˆæœä¸ä½³")
    if avg_memory > 80:
        print("  âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å½±å“æ€§èƒ½")
    if avg_processes > 20:
        print("  âš ï¸  Pythonè¿›ç¨‹è¿‡å¤šï¼Œå¯èƒ½å­˜åœ¨èµ„æºç«äº‰")

def analyze_dataloader_bottleneck():
    """åˆ†ææ•°æ®åŠ è½½å™¨çš„å…·ä½“ç“¶é¢ˆ"""
    print("\n=== æ•°æ®åŠ è½½å™¨ç“¶é¢ˆåˆ†æ ===")
    
    try:
        sys.path.append('./PatchTST_supervised')
        from data_provider.data_loader_for_inference import Dataset_Flight_Inference
        
        # æµ‹è¯•æ•°æ®é›†åˆå§‹åŒ–æ—¶é—´
        print("æµ‹è¯•æ•°æ®é›†åˆå§‹åŒ–æ—¶é—´...")
        start_time = time.time()
        
        dataset = Dataset_Flight_Inference(
            root_path="./PatchTST_supervised/dataset/processed_for_web/",
            data_path="history_data.parquet",
            size=[192, 0, 72],
            features='M',
            target='H',
            scale=True,
            timeenc=1,
            freq='s'
        )
        
        init_time = time.time() - start_time
        print(f"æ•°æ®é›†åˆå§‹åŒ–æ—¶é—´: {init_time:.2f}s")
        print(f"æ•°æ®é›†å¤§å°: {len(dataset)} ä¸ªæ ·æœ¬")
        
        # æµ‹è¯•å•ä¸ªæ ·æœ¬è·å–æ—¶é—´
        print("\næµ‹è¯•å•ä¸ªæ ·æœ¬è·å–æ—¶é—´...")
        sample_times = []
        
        for i in range(100):
            start_time = time.time()
            sample = dataset[i]
            sample_time = time.time() - start_time
            sample_times.append(sample_time)
        
        avg_sample_time = np.mean(sample_times)
        print(f"å¹³å‡å•æ ·æœ¬è·å–æ—¶é—´: {avg_sample_time:.4f}s")
        print(f"å•æ ·æœ¬ååé‡: {1/avg_sample_time:.2f} samples/s")
        
        # åˆ†æç“¶é¢ˆ
        if init_time > 10:
            print("âš ï¸  æ•°æ®é›†åˆå§‹åŒ–æ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½æ˜¯æ–‡ä»¶è¯»å–æˆ–é¢„å¤„ç†ç“¶é¢ˆ")
        if avg_sample_time > 0.01:
            print("âš ï¸  å•æ ·æœ¬è·å–æ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½æ˜¯__getitem__æ–¹æ³•ç“¶é¢ˆ")
        
    except Exception as e:
        print(f"æ•°æ®åŠ è½½å™¨åˆ†æå¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¼€å§‹æ€§èƒ½ç“¶é¢ˆè¯Šæ–­...")
    print(f"ç³»ç»Ÿä¿¡æ¯:")
    print(f"  CPUæ ¸å¿ƒæ•°: {psutil.cpu_count()} (é€»è¾‘)")
    print(f"  ç‰©ç†æ ¸å¿ƒæ•°: {psutil.cpu_count(logical=False)}")
    print(f"  æ€»å†…å­˜: {psutil.virtual_memory().total / (1024**3):.1f} GB")
    print(f"  CUDAå¯ç”¨: {'æ˜¯' if __import__('torch').cuda.is_available() else 'å¦'}")
    
    try:
        # 1. åˆ†ææ•°æ®åŠ è½½å™¨ç“¶é¢ˆ
        analyze_dataloader_bottleneck()
        
        # 2. åˆ†ææ•°æ®åŠ è½½å™¨æ€§èƒ½
        profile_dataloader_performance()
        
        # 3. åˆ†ææ¨¡å‹æ¨ç†æ€§èƒ½
        profile_model_inference()
        
        # 4. ç›‘æ§ç³»ç»Ÿèµ„æº
        monitor_system_during_inference()
        
        print("\nğŸ¯ è¯Šæ–­å®Œæˆï¼")
        print("æ ¹æ®ä»¥ä¸Šåˆ†æç»“æœï¼Œä¸»è¦ç“¶é¢ˆå¯èƒ½åœ¨äº:")
        print("1. æ•°æ®åŠ è½½å™¨é…ç½®ä¸å½“ï¼ˆnum_workers, batch_sizeï¼‰")
        print("2. æ•°æ®é¢„å¤„ç†è¿‡ç¨‹è€—æ—¶")
        print("3. I/Oç“¶é¢ˆï¼ˆæ–‡ä»¶è¯»å–é€Ÿåº¦ï¼‰")
        print("4. å†…å­˜å¸¦å®½é™åˆ¶")
        print("5. CPUæ ¸å¿ƒåˆ©ç”¨ç‡ä¸è¶³")
        
    except Exception as e:
        print(f"è¯Šæ–­è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

if __name__ == "__main__":
    main()
